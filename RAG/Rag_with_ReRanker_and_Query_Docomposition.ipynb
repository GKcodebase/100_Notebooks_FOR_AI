{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0RTZ1dIyB1W"
      },
      "source": [
        "RAG with Reranking and Query Decomposition for 10-K Filings\n",
        "\n",
        "Purpose: Build a prototype Retrieval-Augmented Generation (RAG) system that answers questions using information from 10‑K filings (SEC). This notebook demonstrates how adding a reranker and query decomposition improves retrieval quality and final answers.\n",
        "\n",
        "Example task: Answer the question \"List the major changes that occurred at the company in 2023\" using Tesla's 10‑K. (Note: typos in example queries can be intentional — this notebook shows how the system handles them.)\n",
        "\n",
        "Learning outcomes:\n",
        "- How to create and use a vectorstore built from SEC filings.\n",
        "- How reranking (Flashrank or similar) improves retrieval precision.\n",
        "- How query decomposition (sub-queries) helps gather targeted context for complex questions.\n",
        "- How to assemble a RAG pipeline with LangChain components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FACE-NEZaw16"
      },
      "source": [
        "# How to Run this Notebook\n",
        "\n",
        "Quick start (Colab or local):\n",
        "\n",
        "1. Obtain an OpenAI API key: https://platform.openai.com/settings/api-keys — save it securely.\n",
        "2. In Google Colab: open the left sidebar Secrets (key icon) → `+ Add new secret` → set name `OPENAI_KEY` and paste the key as the value. Enable the secret for this notebook.\n",
        "3. Locally: export the key in your shell before running the notebook:\n",
        "\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-...\"\n",
        "```\n",
        "\n",
        "4. Run the notebook top→down (`Runtime -> Run all` in Colab) or execute cells in order in your environment.\n",
        "\n",
        "Notes:\n",
        "- This notebook builds on an earlier RAG exercise; repeated conceptual explanations are intentionally abbreviated here.\n",
        "- If you lose an API key in the UI, generate a new key — the UI shows keys only once at creation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvPh7tDHyB1Z"
      },
      "source": [
        "# Basic Setup\n",
        "\n",
        "This section installs required packages and prepares the runtime. If you run locally, prefer using a virtual environment (venv / conda) and install the dependencies there.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p5_tpSZrH81"
      },
      "source": [
        "\n",
        "## Install Frameworks\n",
        "\n",
        "Install the Python packages used in this notebook. The provided cell uses `pip` to install LangChain, FAISS, OpenAI client and the optional reranker.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyuvJtpUrEVj"
      },
      "source": [
        "- `langchain`, `langchain_core`, `langchain_community`, `langchain_huggingface`, `langchain_openai` — LangChain core and provider integrations.\n",
        "- `faiss-cpu` — FAISS for similarity search and efficient vector indexes (CPU build).\n",
        "- `openai` — OpenAI Python SDK for embeddings and LLM calls.\n",
        "- `flashrank` — optional reranker used to improve result ordering.\n",
        "\n",
        "Tip: pin versions for reproducible runs when sharing notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm2Y3oLZyB1Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_core langchain_community faiss-cpu openai langchain_openai langchain_huggingface -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_rq1aYHas8L"
      },
      "outputs": [],
      "source": [
        "# New installs\n",
        "%%capture\n",
        "!pip install -q U flashrank  # for Flashrank monkeypatch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu0fuMI12r83",
        "outputId": "6598d7d2-4794-4022-c379-1996d7a16166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2.10\n"
          ]
        }
      ],
      "source": [
        "import importlib.metadata\n",
        "print(importlib.metadata.version(\"flashrank\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSeepaUCY2cA",
        "outputId": "f198a181-13a0-4b36-907e-180dd19f9df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "langchain version: 1.0.5\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "print(f\"langchain version: {langchain.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjjMlvYfyB1a"
      },
      "source": [
        "## API Keys Setup\n",
        "\n",
        "Configure API keys required by the notebook:\n",
        "\n",
        "- `OPENAI_API_KEY` — OpenAI key for embeddings and LLM requests.\n",
        "- (Optional) `HUGGINGFACE_API_KEY` / `COHERE_API_KEY` if you use Hugging Face or Cohere services.\n",
        "\n",
        "In Colab: save keys using the Secrets manager and load them into environment variables. Locally: set environment variables in your shell or use a secrets manager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqwr8z58yB1a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if 'google.colab' in str(get_ipython):\n",
        "    from google.colab import userdata\n",
        "    # Set environment variables\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN_AI_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy9hcf9gJUuF"
      },
      "source": [
        "## Google Drive and Local Paths\n",
        "\n",
        "When running in Colab this section mounts Google Drive and creates paths to persist FAISS indexes and data. If you run locally, update the paths to use a local `./data` or other persistent directory instead of Colab paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqc39aerJTH1",
        "outputId": "126e28fe-a47e-4c65-aa5a-c1a02bd0a860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHCQ_v1ZJg3z"
      },
      "outputs": [],
      "source": [
        "# Set up paths\n",
        "gdrive_root = Path('/content/drive/My Drive')\n",
        "faiss_dir = gdrive_root/\"LLM/RAG/faiss_index\"\n",
        "faiss_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5k6ahgnyB1a"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Import the libraries used by the RAG pipeline: document loaders, text splitters, vectorstores, embedding providers, reranker and LangChain primitives.\n",
        "\n",
        "Key imports are annotated inline in the code cell for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feeydAIryB1a",
        "outputId": "d56c8e82-04d9-470a-b503-4fe718170103"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "# LangChain imports\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.documents import Document\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "from pprint import pprint\n",
        "from typing import List\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS3_BrbYcV8s"
      },
      "outputs": [],
      "source": [
        "# New imports\n",
        "from flashrank import Ranker, RerankRequest\n",
        "import langchain_community.document_compressors.flashrank_rerank as fr_mod\n",
        "fr_mod.RerankRequest = RerankRequest\n",
        "\n",
        "from langchain_community.document_compressors import FlashrankRerank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re7VZjvJbhCT"
      },
      "source": [
        "# Configuration Dictionary\n",
        "\n",
        "The `defaultConfig` dictionary centralizes settings for chunking, embeddings, retrieval, reranking, prompt templates and model choices. Edit these values to adapt behavior (for example, change `chunkSize` or `numRerankedDocuments`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Go6TKSbenl"
      },
      "outputs": [],
      "source": [
        "defaultConfig = {\n",
        "    # Document processing settings\n",
        "    \"chunkSize\": 500,\n",
        "    \"chunkOverlap\": 50,\n",
        "    \"userAgentHeader\": \"YourCompany-ResearchBot/1.0 (your@email.com)\",\n",
        "\n",
        "    # embedding model\n",
        "    \"embeddingModel\": \"BAAI/bge-base-en-v1.5\",\n",
        "\n",
        "    # Vector store settings\n",
        "    \"numRetrievedDocuments\": 12,\n",
        "\n",
        "    # Document formater settings\n",
        "    \"numSelectedDocuments\": 5,\n",
        "\n",
        "    # Reranker setting\n",
        "    \"rerankerType\": \"flashrank\",\n",
        "    \"rerankerModel\": \"ms-marco-TinyBERT-L-2-v2\",\n",
        "    \"numRerankedDocuments\": 5,\n",
        "\n",
        "    # Model settings\n",
        "    \"ragAnswerModel\": \"gpt-4o\",\n",
        "    \"ragAnswerModelTemeprature\": 0.7,\n",
        "\n",
        "    # URLs to process\n",
        "    \"companyFilingUrls\": [\n",
        "        (\"Tesla\", \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\")\n",
        "    ],\n",
        "\n",
        "    # RAG prompt template\n",
        "    \"ragPromptTemplate\": \"\"\"\n",
        "    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\n",
        "    Provide a detailed answer with thorough explanations, avoiding summaries.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\",\n",
        "\n",
        "    # Query Decomposer settings\n",
        "    'queryDecomposerModel': \"gpt-4o-mini\",\n",
        "    'queryDecomposerModelTemperature': 0.7,\n",
        "\n",
        "    # SubQuery prompt template\n",
        "    \"subqueryPromptTemplate\": \"\"\"\n",
        "    Break down the `Question` into multiple sub-queries. Use the guidelines given below to help in the task.\n",
        "\n",
        "    1. The set of sub-queries together capture the complete information needed to answer the question.\n",
        "    2. Each sub-query should ask for just one piece of information about one specific company.\n",
        "    3. For each sub-query, only mention the information you're trying to get. Don't use verbs like \"retrieve\" or \"find\".\n",
        "    4. Include the company name mentioned in each sub-query.\n",
        "    5. Do not include any references to data sources in your sub-queries.\n",
        "\n",
        "    Enclose the sub-query in angle brackets. For example:\n",
        "    <sub-query 1>\n",
        "    <sub-query 2>\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Begin:\n",
        "    \"\"\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aBC2APb9pWc"
      },
      "outputs": [],
      "source": [
        "config = defaultConfig.copy() # Creates a separate copy of the default configuration dictionary (defaultConfig) so that any subsequent changes won't alter the original default settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySjUhhS-yB1b"
      },
      "source": [
        "# Load Vector Store\n",
        "\n",
        "This cell loads a prebuilt FAISS vectorstore from disk. If you don't have an existing vectorstore, run the ingestion/indexing notebook first to build it from SEC filings.\n",
        "\n",
        "Ensure the `faiss_dir` path matches where the index was saved and that the embedding function used here matches the embeddings used when creating the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geunm-dJyB1b"
      },
      "outputs": [],
      "source": [
        "embedding_model_name = config.get('embeddingModelName', 'text-embedding-3-small')\n",
        "\n",
        "if embedding_model_name.startswith(\"text-embedding\"):\n",
        "    # Use OpenAIEmbeddings for OpenAI models\n",
        "    embeddingFunction = OpenAIEmbeddings(model=embedding_model_name)\n",
        "else:\n",
        "    # Use HuggingFaceEmbeddings for other models (assuming they are from HuggingFace)\n",
        "    embeddingFunction = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "loaded_vectorstore = FAISS.load_local(str(faiss_dir), embeddingFunction, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IpelndxxEOz"
      },
      "source": [
        "# Retriever\n",
        "\n",
        "The retriever fetches candidate documents by semantic similarity. Optionally, a reranker reorders those candidates using a more precise model to improve the top results' relevance.\n",
        "\n",
        "The `create_retriever_with_reranking` function builds a base FAISS retriever and conditionally wraps it with a reranking compressor (Flashrank) to refine results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEl518MTihdV"
      },
      "outputs": [],
      "source": [
        "def create_retriever_with_reranking(vectorstore, config, use_reranking = True ):\n",
        "    \"\"\"\n",
        "    Creates a retriever with optional reranking capability.\n",
        "\n",
        "    Args:\n",
        "        vectorstore: The vector store to retrieve from\n",
        "        config: Configuration dictionary\n",
        "        use_reranking: Whether to use reranking (default: True)\n",
        "\n",
        "    Returns:\n",
        "        A retriever (either basic or with reranking)\n",
        "    \"\"\"\n",
        "    # Create base retriever\n",
        "    base_retriever = vectorstore.as_retriever(\n",
        "        search_kwargs={\"k\": config.get(\"numRetrievedDocuments\", 12)}\n",
        "    )\n",
        "\n",
        "    # If reranking is disabled, return the base retriever\n",
        "    if not use_reranking:\n",
        "        return base_retriever\n",
        "\n",
        "    try:\n",
        "        # Initialize the reranker\n",
        "        model_name = config.get(\"rerankerModel\", \"ms-marco-TinyBERT-L-2-v2\")\n",
        "        top_n = int(config.get(\"numRerankedDocuments\", 5))\n",
        "        ranker_client = Ranker(model_name=model_name)\n",
        "        reranker = FlashrankRerank(client=ranker_client, model=model_name, top_n=top_n)\n",
        "\n",
        "\n",
        "        # Create and return the enhanced retrieval system\n",
        "        return ContextualCompressionRetriever(base_retriever=base_retriever, base_compressor=reranker)\n",
        "    except Exception as e:\n",
        "        print(f\" Error setting up reranker: {e}\")\n",
        "        print(\"Falling back to base retriever.\")\n",
        "        return base_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRde2x2gUs35",
        "outputId": "640eabcd-8366-41f1-e2fc-63555f2a882b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ms-marco-TinyBERT-L-2-v2.zip: 100%|██████████| 3.26M/3.26M [00:00<00:00, 13.8MiB/s]\n"
          ]
        }
      ],
      "source": [
        "retriever = create_retriever_with_reranking(loaded_vectorstore, config, use_reranking = True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArrYmE62iIG0"
      },
      "source": [
        "# Query Decomposer\n",
        "\n",
        "For complex questions, the decomposer breaks a query into focused sub-queries. Each sub-query retrieves documents that are then merged to form a richer context for the final answer. This improves coverage and reduces hallucination risk for multi-part questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsQgdjSMfQAn"
      },
      "outputs": [],
      "source": [
        "def create_decomposer(config):\n",
        "    prompt = PromptTemplate.from_template(config[\"subqueryPromptTemplate\"])\n",
        "    llm = ChatOpenAI(\n",
        "        model=config[\"queryDecomposerModel\"],\n",
        "        temperature=config[\"queryDecomposerModelTemperature\"]\n",
        "    )\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    def decompose_query(question):\n",
        "        response = chain.invoke({\"question\": question})\n",
        "        return re.findall(r'<(.*?)>', response, re.DOTALL)\n",
        "\n",
        "    return decompose_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Gxy3YBfSUo"
      },
      "outputs": [],
      "source": [
        "decomposer = create_decomposer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "5vfF7FYvfbHL",
        "outputId": "70afb2a7-5680-40f0-e28d-9663b9a1a3d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>create_decomposer.&lt;locals&gt;.decompose_query</b><br/>def decompose_query(question)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-2032634301.py</a>&lt;no docstring&gt;</pre></div>"
            ],
            "text/plain": [
              "<function __main__.create_decomposer.<locals>.decompose_query(question)>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decomposer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xOFgE0nyB1c"
      },
      "source": [
        "# RAG Chain\n",
        "\n",
        "RAG pipeline components:\n",
        "1. **Retriever** — obtains semantically relevant documents (with optional reranking).\n",
        "2. **Document Formatter** — concatenates or structures documents into a `context` for the LLM.\n",
        "3. **LLM** — generates the answer conditioned on the `context` and `question`.\n",
        "4. **Prompt Template** — controls how `context` and `question` are presented to the LLM.\n",
        "\n",
        "The notebook shows how to build a flexible chain that supports both single-shot retrieval and the decomposition + aggregation flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3qPMDemyMEz"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "\n",
        "# --- Prompt and LLM ---\n",
        "def create_prompt(config):\n",
        "    return PromptTemplate.from_template(config[\"ragPromptTemplate\"])\n",
        "\n",
        "\n",
        "def create_llm(config):\n",
        "    return ChatOpenAI(\n",
        "        model=config[\"ragAnswerModel\"],\n",
        "        temperature=config[\"ragAnswerModelTemeprature\"]\n",
        "    )\n",
        "\n",
        "def build_rag_chain(retriever, format_docs_fn, prompt, llm):\n",
        "    return (\n",
        "        {\"context\": retriever | format_docs_fn, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aQBERyl_aRu"
      },
      "outputs": [],
      "source": [
        "def run_pipeline_flexible(question, retriever, decomposer, config):\n",
        "    prompt = create_prompt(config)\n",
        "    llm = create_llm(config)\n",
        "\n",
        "    if decomposer is not None:\n",
        "        subqueries = decomposer(question)\n",
        "        all_docs = []\n",
        "        seen_source_ids = set()\n",
        "\n",
        "        for subq in subqueries:\n",
        "            print(f\"Subquery: {subq}\")\n",
        "            # Use retriever.invoke() instead of retriever.get_relevant_documents()\n",
        "            docs = retriever.invoke(subq)\n",
        "            print(f\"Retrieved {len(docs)} documents for subquery\")\n",
        "\n",
        "            for doc in docs:\n",
        "                source_id = doc.metadata.get(\"source_id\", \"\")\n",
        "                if source_id and source_id not in seen_source_ids:\n",
        "                    all_docs.append(doc)\n",
        "                    seen_source_ids.add(source_id)\n",
        "                    print(f\"Added document: {source_id}\")\n",
        "\n",
        "        print(f\"Total unique documents: {len(all_docs)}\")\n",
        "        company_counts = {}\n",
        "        for doc in all_docs:\n",
        "            company = doc.metadata.get(\"company\", \"unknown\")\n",
        "            company_counts[company] = company_counts.get(company, 0) + 1\n",
        "        print(f\"Documents by company: {company_counts}\")\n",
        "\n",
        "        context = format_docs(all_docs)\n",
        "        print(f\"Context size: {len(context)} characters\")\n",
        "\n",
        "        chain = prompt | llm | StrOutputParser()\n",
        "        answer = chain.invoke({\"context\": context, \"question\": question})\n",
        "        return answer\n",
        "    else:\n",
        "        chain = build_rag_chain(retriever, format_docs, prompt, llm)\n",
        "        return chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6I77FOmKEN8"
      },
      "source": [
        "# Run the Pipeline to Answer Questions\n",
        "\n",
        "Provide a `question` and execute `run_pipeline_flexible(question, retriever, decomposer, config)`. If decomposer is enabled, the function will print each subquery and the number of retrieved/merged documents before returning the final answer generated by the LLM.\n",
        "\n",
        "Tips:\n",
        "- Start with simple factual questions to validate retrieval quality.\n",
        "- Inspect retrieved documents when results seem off — reranker and retriever settings can be tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se9IF2k7KEN8"
      },
      "outputs": [],
      "source": [
        "question =\"How do Tesla and GM's approaches to manufacturing and production compare, particularly for electric vehicles? Where are their vehicles produced? What are the saftey standards followed in their vehicles?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaH77dpBKEN8",
        "outputId": "670acc56-b179-4ec9-f27f-6a8351d1cfc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"How do Tesla and GM's approaches to manufacturing and production compare, \"\n",
            " 'particularly for electric vehicles? Where are their vehicles produced? What '\n",
            " 'are the saftey standards followed in their vehicles?')\n"
          ]
        }
      ],
      "source": [
        "pprint(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9cjaQm_KEN8",
        "outputId": "a9ae4694-bae4-4287-a336-f12883ea09f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subquery: Tesla manufacturing and production approach for electric vehicles\n",
            "Retrieved 5 documents for subquery\n",
            "Subquery: GM manufacturing and production approach for electric vehicles\n",
            "Retrieved 5 documents for subquery\n",
            "Subquery: Tesla vehicle production locations\n",
            "Retrieved 5 documents for subquery\n",
            "Subquery: GM vehicle production locations\n",
            "Retrieved 5 documents for subquery\n",
            "Subquery: Tesla safety standards followed in vehicles\n",
            "Retrieved 5 documents for subquery\n",
            "Subquery: GM safety standards followed in vehicles\n",
            "Retrieved 5 documents for subquery\n",
            "Total unique documents: 0\n",
            "Documents by company: {}\n",
            "Context size: 0 characters\n"
          ]
        }
      ],
      "source": [
        "answer = run_pipeline_flexible(question, retriever, decomposer, config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
