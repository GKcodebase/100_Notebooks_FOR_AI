{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0RTZ1dIyB1W"
      },
      "source": [
        "# Simple RAG for 10-K Filings\n",
        "\n",
        "This notebook demonstrates a compact Retrieval-Augmented Generation (RAG) pipeline that answers targeted questions using public 10‑K filings from the U.S. Securities and Exchange Commission (SEC).\n",
        "\n",
        "Scope: process a 10‑K document, split it into chunks, create embeddings, store vectors in FAISS, retrieve relevant passages, and generate answers with a large language model.\n",
        "\n",
        "Learning goals:\n",
        "- Load and inspect 10‑K filings programmatically.\n",
        "- Split long documents into overlapping chunks for retrieval.\n",
        "- Produce embeddings and build a FAISS vector store.\n",
        "- Implement a retriever + prompt template to feed context to an LLM.\n",
        "- Generate and post-process model answers grounded in source text.\n",
        "\n",
        "Definitions:\n",
        "- **RAG (Retrieval-Augmented Generation)**: a pattern that augments a generative model with retrieved documents so answers are grounded in external sources.\n",
        "- **10‑K**: an annual report that public companies file with the SEC describing business operations, risks, and financials.\n",
        "\n",
        "**Objective**: Use the specified source document(s) to answer the question below, and show the supporting passages used to form the answer.\n",
        "\n",
        "Question: \"What technological advancements were made in the batteries used in Tesla's electric vehicles?\"\n",
        "Source: Tesla 2023 Form 10‑K (SEC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FACE-NEZaw16"
      },
      "source": [
        "# How to Run this Notebook\n",
        "\n",
        "Follow these steps to prepare the environment and run all cells. This notebook expects an OpenAI API key to generate embeddings and call the model.\n",
        "\n",
        "1. Get an OpenAI API key:\n",
        "   - Visit: https://platform.openai.com/account/api-keys and create a new API key.\n",
        "   - Save it securely; you will not be able to view the key again after creation.\n",
        "\n",
        "2. Add the API key to your notebook's Secrets manager:\n",
        "   - Click the Secrets / Keys icon in the notebook UI (left sidebar).\n",
        "   - Click '+ Add new secret'.\n",
        "   - **Name**: `OPEN_AI_KEY` (the notebook's code reads this name).\n",
        "   - **Value**: paste your OpenAI API key.\n",
        "\n",
        "   Note: the code in this notebook maps the secret `OPEN_AI_KEY` into the environment variable `OPENAI_API_KEY` before using OpenAI clients.\n",
        "\n",
        "3. Enable access for this notebook to the secret (toggle access permissions in the Secrets UI).\n",
        "\n",
        "4. Optional: if you prefer a different secret name (for example `OPENAI_API_KEY`), update the small setup cell that reads the secret or add both names to the Secrets manager.\n",
        "\n",
        "5. Security best practices:\n",
        "   - Never commit API keys to source control.\n",
        "   - Use scoped/organizational keys when possible and rotate keys periodically.\n",
        "\n",
        "6. Run the notebook:\n",
        "   - Click 'Runtime' → 'Run all' (or run cells sequentially).\n",
        "\n",
        "If you encounter authentication errors, verify the secret name and that the environment variable `OPENAI_API_KEY` is being set in the setup cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvPh7tDHyB1Z"
      },
      "source": [
        "## Basic Setup\n",
        "\n",
        "This notebook was developed for Python 3.10+ and is intended to run in a Jupyter / Colab-like environment.\n",
        "Recommended workflow:\n",
        "- Use a virtual environment (`venv` or `conda`) to isolate dependencies.\n",
        "- On macOS (zsh) activate your environment before running the notebook.\n",
        "- If you use Google Colab, follow the 'How to Run this Notebook' cell to add secrets.\n",
        "\n",
        "Quick setup commands (macOS, zsh):\n",
        "```bash\n",
        "python3 -m venv .venv\n",
        "source .venv/bin/activate\n",
        "pip install -U pip\n",
        "```\n",
        "\n",
        "If you plan to run larger models locally, ensure appropriate hardware (GPU + CUDA) and install GPU-compatible builds of libraries. Otherwise, use CPU-only packages or cloud APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p5_tpSZrH81"
      },
      "source": [
        "## Install Frameworks\n",
        "\n",
        "Install the Python packages required by this notebook. The provided pip command installs the main dependencies used in the examples below. Pin versions for reproducibility if needed.\n",
        "\n",
        "Run this cell to install dependencies (works in Colab and most local environments):\n",
        "\n",
        "```bash\n",
        "!pip install langchain langchain_core langchain_community faiss-cpu openai langchain_openai langchain_huggingface -U\n",
        "```\n",
        "\n",
        "Note: On local macOS systems you may prefer `conda install -c conda-forge faiss-cpu` if pip wheels fail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyuvJtpUrEVj"
      },
      "source": [
        "### Key libraries used\n",
        "\n",
        "- **langchain / langchain_core / langchain_community**: A framework for composing LLM pipelines, chains, document loaders, and retrievers.\n",
        "- **langchain_huggingface**: Integration helpers for using HuggingFace embeddings and models within LangChain workflows.\n",
        "- **faiss-cpu**: Facebook AI Similarity Search — an in-memory vector index for fast nearest-neighbor retrieval (CPU build). Use GPU FAISS for large-scale datasets if available.\n",
        "- **openai / langchain_openai**: Official OpenAI Python client and LangChain's OpenAI integration for embeddings and chat/completion calls.\n",
        "\n",
        "Optional utilities: `transformers` and `sentence-transformers` if you want to run local HuggingFace embedding models instead of using OpenAI (lower API cost, potentially higher runtime).\n",
        "\n",
        "Advice: Use OpenAI embeddings for high-quality vectors but be mindful of API costs. HuggingFace models are a viable local alternative for experimentation or cost-sensitive workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm2Y3oLZyB1Z"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.9' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "# Install necessary libraries.\n",
        "# We use %%capture to suppress the extensive output logs during installation.\n",
        "!pip install langchain langchain_core langchain_community faiss-cpu openai langchain_openai langchain_huggingface -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmlVQU52aGLF",
        "outputId": "818893e4-20f6-4438-bdfa-979532344c9a"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "import langchain_core\n",
        "import langchain_community\n",
        "import openai\n",
        "\n",
        "# Verify installed versions to ensure compatibility\n",
        "print(f\"langchain version: {langchain.__version__}\")\n",
        "print(f\"langchain_core version: {langchain_core.__version__}\")\n",
        "print(f\"langchain_community version: {langchain_community.__version__}\")\n",
        "# FAISS and langchain_openai do not have a standard __version__ attribute exposed here\n",
        "print(f\"openai version: {openai.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjjMlvYfyB1a"
      },
      "source": [
        "## API Keys Setup\n",
        "\n",
        "This notebook uses API keys to access external services (OpenAI for embeddings/LLM calls; optional HuggingFace for embeddings/models). Below are recommended secret names and examples for Colab and local runs.\n",
        "\n",
        "Recommended secret / environment variable names:\n",
        "- `OPENAI_API_KEY` — the standard environment variable used by OpenAI client libraries.\n",
        "- `HUGGINGFACEHUB_API_TOKEN` — optional token for HuggingFace Hub access if you use HuggingFace models.\n",
        "\n",
        "Colab / Notebook Secrets (recommended):\n",
        "- Add a secret named `OPEN_AI_KEY` or `OPENAI_API_KEY` in the notebook Secrets UI.\n",
        "- The notebook contains a small setup cell that maps `OPEN_AI_KEY` → `OPENAI_API_KEY` for compatibility. If you prefer, add `OPENAI_API_KEY` directly and update the setup cell accordingly.\n",
        "\n",
        "Example: local environment (macOS / zsh):\n",
        "```bash\n",
        "export OPENAI_API_KEY=\"sk-...\"\n",
        "export HUGGINGFACEHUB_API_TOKEN=\"hf_...\"  # optional\n",
        "```\n",
        "\n",
        "Example: Google Colab Secrets mapping (the notebook does this automatically if you used the Secrets UI):\n",
        "```python\n",
        "# inside the notebook setup cell\n",
        "import os\n",
        "from google.colab import userdata  # only available in Colab\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_AI_KEY')  # maps the secret to the env var\n",
        "```\n",
        "\n",
        "Security notes:\n",
        "- Never hard-code API keys in notebooks or commit them to source control.\n",
        "- Prefer notebook Secret managers or environment variables for runtime-only access.\n",
        "- Rotate keys regularly and limit scope where possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqwr8z58yB1a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Load the API key from Colab User Data (Secrets)\n",
        "if 'google.colab' in str(get_ipython):\n",
        "    from google.colab import userdata\n",
        "    # Set the environment variable that OpenAI libraries expect\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN_AI_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5k6ahgnyB1a"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "This cell imports the Python modules used in the RAG pipeline. Short descriptions help you understand each component's role:\n",
        "\n",
        "- `WebBaseLoader`: downloads and parses web pages (used to fetch SEC filing HTML).\n",
        "- `RecursiveCharacterTextSplitter`: splits long documents into overlapping chunks for retrieval.\n",
        "- `PromptTemplate`: constructs prompt strings with placeholders for question/context.\n",
        "- `FAISS`: in-memory vector index used as the vector store for nearest-neighbor search.\n",
        "- `ChatOpenAI`: LangChain wrapper to call OpenAI chat/completion models (used for answer generation).\n",
        "- `StrOutputParser`: converts LLM structured output into a plain string for display.\n",
        "- `RunnablePassthrough`: a pipeline utility that forwards inputs unchanged (useful for questions).\n",
        "- `Document`: LangChain document data type that holds `page_content` and `metadata`.\n",
        "- `HuggingFaceEmbeddings`: optional embeddings implementation if you use HuggingFace models locally.\n",
        "- `pprint`: pretty-print helper for readable debug output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feeydAIryB1a",
        "outputId": "ad00a544-0ffa-45d2-80ed-06fc3f1c6e5f"
      },
      "outputs": [],
      "source": [
        "# LangChain imports for RAG components\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.documents import Document\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re7VZjvJbhCT"
      },
      "source": [
        "# Configuration Dictionary\n",
        "\n",
        "This dictionary centralizes settings used throughout the notebook so you can tune behavior from one place. Below are the most important fields and recommended defaults with rationale:\n",
        "\n",
        "- `chunkSize` (int, default 500): target token/character length for each text chunk. Larger chunks preserve more context but increase embedding cost and may reduce retrieval precision. Start with 400–800 for SEC filings.\n",
        "- `chunkOverlap` (int, default 50): number of overlapping characters between adjacent chunks to preserve cross-boundary context. Typical values: 20–200 depending on chunkSize.\n",
        "- `userAgentHeader` (str): custom User-Agent header for web requests. Use a descriptive string (e.g., `YourCompany-ResearchBot/1.0 (email@example.com)`) to comply with site policies and avoid being blocked.\n",
        "- `embeddingModelName` (str): embedding model identifier (e.g., `text-embedding-3-small`). Use high-quality embeddings for better retrieval; choose cheaper or local models for experimentation.\n",
        "- `numRetrievedDocuments` (int): how many candidate passages the retriever returns (k). Higher `k` increases recall but may add noise—common values 3–10.\n",
        "- `numSelectedDocuments` (int): how many documents to include when formatting the final `context` passed to the LLM. Keep this small enough to fit the model context window (2–6).\n",
        "- `ragAnswerModel` (str): name of the LLM used for answer generation. Choose a model with enough context and reasoning capacity.\n",
        "- `ragAnswerModelTemeprature` (float): sampling temperature for generation (0.0–1.0). Lower values produce more deterministic, factual answers; higher values increase creativity.\n",
        "- `companyFilingUrls` (list of tuples): target sources as `(company_name, url)` tuples. Use canonical SEC URLs or local file paths if you prefer offline files.\n",
        "- `ragPromptTemplate` (str): the prompt template used to ask the LLM. Keep prompts explicit about using only provided context and requesting source citations if desired.\n",
        "\n",
        "Tuning tips:\n",
        "- If answers are hallucinating, reduce `numSelectedDocuments` and lower temperature to 0.0–0.3, and ensure the retriever returns higher-quality passages (adjust embeddings or `k`).\n",
        "- For long documents, increase `chunkOverlap` slightly to preserve sentence continuity across chunks.\n",
        "- For cost-sensitive runs, use smaller embedding models or run HuggingFace embeddings locally; measure retrieval F1/precision to select tradeoffs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C-Go6TKSbenl"
      },
      "outputs": [],
      "source": [
        "defaultConfig = {\n",
        "    # Document processing settings\n",
        "    \"chunkSize\": 500,          # Size of each text chunk\n",
        "    \"chunkOverlap\": 50,        # Overlap between chunks to preserve context\n",
        "    \"userAgentHeader\": \"YourCompany-ResearchBot/1.0 (your@email.com)\",\n",
        "\n",
        "    # Embedding model configuration\n",
        "    \"embeddingModelName\": \"text-embedding-3-small\",  # OpenAI's efficient embedding model\n",
        "\n",
        "    # Vector store retrieval settings\n",
        "    \"numRetrievedDocuments\": 5,\n",
        "\n",
        "    # Document formatter settings\n",
        "    \"numSelectedDocuments\": 5,\n",
        "\n",
        "    # LLM generation settings\n",
        "    \"ragAnswerModel\": \"gpt-4o\",\n",
        "    \"ragAnswerModelTemeprature\": 0.7,\n",
        "\n",
        "    # Target data source\n",
        "    \"companyFilingUrls\": [\n",
        "        (\"Tesla\", \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\")\n",
        "    ],\n",
        "\n",
        "    # RAG prompt template\n",
        "    \"ragPromptTemplate\": \"\"\"\n",
        "    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\n",
        "    Provide a detailed answer with thorough explanations, avoiding summaries.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7aBC2APb9pWc"
      },
      "outputs": [],
      "source": [
        "# Create a working copy of the configuration to avoid accidental modification of defaults\n",
        "config = defaultConfig.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySjUhhS-yB1b"
      },
      "source": [
        "# Load Document\n",
        "\n",
        "This section demonstrates loading the target 10‑K filing from the SEC using LangChain's `WebBaseLoader` and best practices for responsible web access.\n",
        "\n",
        "About `WebBaseLoader`:\n",
        "- `WebBaseLoader` fetches a URL and extracts the page content into a LangChain `Document` object (with `page_content` and `metadata`).\n",
        "- It is convenient for single-page HTML sources such as SEC filings, but may require HTML cleaning depending on the site structure.\n",
        "\n",
        "User-Agent and polite scraping:\n",
        "- Use a clear `User-Agent` (configured via `userAgentHeader` in `config`) so the server can identify your requests (e.g., `MyOrg-ResearchBot/1.0 (email@example.com)`).\n",
        "- Respect `robots.txt`, rate limits, and site terms of service. For the SEC and many public data sources a polite scraping cadence is expected.\n",
        "- If you expect many requests or bulk downloads, prefer official APIs or bulk data feeds where available (SEC provides bulk access for filings).\n",
        "\n",
        "Caching and reproducibility:\n",
        "- For repeatable experiments, cache downloaded documents locally instead of re-fetching on every run. This reduces network load and avoids accidental rate-limiting.\n",
        "- Consider saving the raw HTML or extracted text alongside metadata (source URL and retrieval timestamp) for traceability.\n",
        "\n",
        "Alternatives and fallbacks:\n",
        "- If the target page structure causes noisy HTML extraction, save the filing locally (or use the SEC bulk-download) and use a file loader instead of `WebBaseLoader`.\n",
        "- For large-scale crawling use robust tools (Scrapy, newspaper3k, or custom parsers) and respect site policies.\n",
        "\n",
        "The code cell below initializes `WebBaseLoader` with a custom `User-Agent` and loads the Tesla 10‑K filing into a `Document` object for downstream processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geunm-dJyB1b",
        "outputId": "fc246279-97f1-4856-f408-b24b5fb7d5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 document(s) with 422473 characters\n"
          ]
        }
      ],
      "source": [
        "# Extract URL and Company Name from config\n",
        "url = config[\"companyFilingUrls\"][0][1]\n",
        "company = config[\"companyFilingUrls\"][0][0]\n",
        "\n",
        "# Initialize the WebBaseLoader with a proper User-Agent\n",
        "loader = WebBaseLoader(\n",
        "    url,\n",
        "    header_template={'User-Agent': config[\"userAgentHeader\"]}\n",
        ")\n",
        "\n",
        "# Load the content\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} document(s) with {len(docs[0].page_content)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ODUtml4dsxS",
        "outputId": "ea80752a-df7f-4c69-8b85-5d5ad345d1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> 1\n"
          ]
        }
      ],
      "source": [
        "# Verify the type and count of loaded documents\n",
        "print(type(docs), len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "npQ9x1dCeUrI",
        "outputId": "63b43b7b-0544-4fa2-8d79-11fdb374582d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.documents.base.Document</b><br/>def __init__(page_content: str, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/documents/base.py</a>Class for storing a piece of text and associated metadata.\n",
              "\n",
              "!!! note\n",
              "    `Document` is for **retrieval workflows**, not chat I/O. For sending text\n",
              "    to an LLM in a conversation, use message types from `langchain.messages`.\n",
              "\n",
              "Example:\n",
              "    ```python\n",
              "    from langchain_core.documents import Document\n",
              "\n",
              "    document = Document(\n",
              "        page_content=&quot;Hello, world!&quot;, metadata={&quot;source&quot;: &quot;https://example.com&quot;}\n",
              "    )\n",
              "    ```</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 283);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "langchain_core.documents.base.Document"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the type of a single document object\n",
        "type(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdldTJQKeXM2",
        "outputId": "102afd3d-39d9-4f4c-cbb4-4ad45603b257"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm',\n",
              " 'title': 'tsla-20231231',\n",
              " 'language': 'No language found.'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the metadata of the loaded document\n",
        "docs[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5U3XKRRufJuQ",
        "outputId": "f9d601e6-301e-4c9f-f09e-45578f6870d8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntsla-20231231false00013186052023FYhttp://fasb.org/us-gaap/2023#AccountingStandardsUpdate202006Membe'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preview the first 100 characters of the raw content\n",
        "docs[0].page_content[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4hlbpxQyB1b"
      },
      "source": [
        "# Split Document into Chunks\n",
        "\n",
        "Long documents (like 10‑Ks) must be split into smaller chunks before creating embeddings and indexing. This cell uses `RecursiveCharacterTextSplitter` to produce overlapping chunks that preserve local context.\n",
        "\n",
        "Key concepts and tradeoffs:\n",
        "- `chunkSize` (characters or tokens): larger chunks keep more context per vector but increase embedding cost and reduce the number of distinct vectors (lower recall). Smaller chunks create more vectors (higher recall) but each vector holds less context (may fragment information).\n",
        "- `chunkOverlap`: overlapping characters between adjacent chunks. Overlap helps preserve sentence continuity and avoids cutting important phrases; typical values are 10–30% of `chunkSize` (e.g., 50 for 500).\n",
        "- Tokens vs characters: many splitters work in characters; when working with token-limited models, approximate tokens ≈ characters/4 as a rough heuristic, or use a tokenizer to be precise.\n",
        "\n",
        "Practical recommendations for SEC filings:\n",
        "- Start with `chunkSize=500` and `chunkOverlap=50` (good default used in this notebook).\n",
        "- If retrieval returns overly broad passages, reduce `chunkSize` (e.g., 300–400) to increase precision.\n",
        "- If answers cut sentences or lose context at boundaries, increase `chunkOverlap` to 100–200 or switch to a sentence-aware splitter.\n",
        "\n",
        "Measuring effects:\n",
        "- Evaluate retrieval quality by manually inspecting top-k retrieved chunks for a sample of queries.\n",
        "- Track metrics such as whether the supporting passage contains a direct answer (precision) and whether the retriever returns at least one relevant passage (recall).\n",
        "\n",
        "Implementation note:\n",
        "- The `RecursiveCharacterTextSplitter` used below balances multiple separators (newlines, sentences) to avoid arbitrary cuts and produce readable chunks for embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-Y9m7Wn3yB1c"
      },
      "outputs": [],
      "source": [
        "# Initialize the text splitter with config parameters\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=config[\"chunkSize\"],\n",
        "    chunk_overlap=config[\"chunkOverlap\"]\n",
        ")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "chunks = splitter.transform_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAxl1ACQpOX3",
        "outputId": "4f80cbd5-7918-430a-b65c-82150b2e2236"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(list, 942)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify the number of resulting chunks\n",
        "type(chunks), len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cwP-ljBpPkf",
        "outputId": "95fbc424-d79a-45d0-e9dc-39e6415a7df5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm',\n",
              " 'title': 'tsla-20231231',\n",
              " 'language': 'No language found.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect metadata of the first chunk\n",
        "chunks[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "u_XFy5jCqNxV",
        "outputId": "740d542f-a7c3-4e8f-ffe1-e1a565b4f4aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dangerous aspects of road travel much like the system that airplane pilots use, when conditions permit. As with other vehicle systems, we improve these functions in our vehicles over time through over-the-air updates.We intend to establish in the future an autonomous Tesla ride-hailing network, which we expect would also allow us to access a new customer base even as modes of transportation evolve.We are also applying our artificial intelligence learnings from self-driving technology to the'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect content of a specific chunk (e.g., the 100th chunk)\n",
        "chunks[100].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "vZEXr8e8qTJj",
        "outputId": "dfe9d906-cdbd-42e1-e896-59ae28fff7a1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'learnings from self-driving technology to the field of robotics, such as through Optimus, a robotic humanoid in development, which is controlled by the same AI system. 5Table of ContentsEnergy Generation and StorageEnergy Storage ProductsWe leverage many of the component-level technologies from our vehicles in our energy storage products. By taking a modular approach to the design of battery systems, we can optimize manufacturing capacity of our energy storage products. Additionally, our'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the next chunk to observe the overlap\n",
        "chunks[101].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I7FQBcXUpNOB"
      },
      "outputs": [],
      "source": [
        "# Enrich metadata: Add the company name to each chunk for better traceability\n",
        "for chunk in chunks:\n",
        "    chunk.metadata[\"company\"] = company"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU1vK18mqmhN",
        "outputId": "6ef56cb8-4f12-456a-a74d-628f8a15cf31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm',\n",
              " 'title': 'tsla-20231231',\n",
              " 'language': 'No language found.',\n",
              " 'company': 'Tesla'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify metadata update\n",
        "chunks[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQurkMHryB1c"
      },
      "source": [
        "OpenAIEmbeddings is a LangChain wrapper that uses OpenAI's embedding API to generate vector embeddings. This requires an OpenAI API key to be set in your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF4Z3HD4vadw"
      },
      "source": [
        "## Generate Embeddings\n",
        "\n",
        "Embeddings convert text into numeric vectors that capture semantic meaning. These vectors enable similarity search (retrieval) by comparing distances between vectors instead of raw text matching.\n",
        "\n",
        "**Theoretical Foundation: Embeddings as Semantic Space**\n",
        "\n",
        "Modern embeddings (produced by transformers like BERT or GPT-based models) map text into a high-dimensional vector space (e.g., 1536 dimensions for OpenAI's embeddings) where:\n",
        "- **Semantically similar texts** have vectors that are close together (small distance).\n",
        "- **Semantically different texts** have vectors that are far apart (large distance).\n",
        "- **Vector operations** are meaningful: the vector for \"king\" minus \"man\" plus \"woman\" is close to \"queen\" (though this is an idealized view).\n",
        "\n",
        "This property is what makes similarity search work: to find documents relevant to a query, we compute the distance (typically L2 Euclidean or cosine similarity) between the query vector and all document vectors, then return the nearest neighbors.\n",
        "\n",
        "**Options in this notebook**:\n",
        "- **OpenAIEmbeddings**: high-quality, managed embeddings provided by OpenAI. Easy to use and frequently produce better downstream retrieval accuracy, but incur API cost and latency.\n",
        "- **HuggingFaceEmbeddings**: local or hosted models from HuggingFace (via `sentence-transformers` or `transformers`). Lower API cost (can run locally) but may require more compute and tuning.\n",
        "\n",
        "**Practical Considerations and Tradeoffs**:\n",
        "- **Quality vs Cost**: OpenAI's embeddings often give better retrieval for small collections; for large-scale or cost-sensitive workflows, consider local HuggingFace models or smaller OpenAI models.\n",
        "- **Dimensionality**: embedding dimension (e.g., 1536) affects index size and similarity behavior. Keep consistent embedding models for indexing and querying. Mismatched dimensions will cause errors.\n",
        "- **Batching & Rate Limits**: Generate embeddings in batches to reduce per-call overhead and respect API rate limits. For 10k+ documents, batch requests in groups of 100–1000 to minimize roundtrips.\n",
        "- **Caching**: Cache embeddings locally in JSON or pickle format to avoid repeated calls for the same text, reducing costs and latency on subsequent runs.\n",
        "- **Normalization**: Some retrieval methods benefit from L2-normalized vectors (each vector has length 1). FAISS supports this via IndexFlatIP (inner product); check your index configuration.\n",
        "\n",
        "**Distance Metrics** (related to embeddings):\n",
        "- **L2 Distance (Euclidean)**: $d(\\mathbf{u}, \\mathbf{v}) = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2}$ — widely used, works well with FAISS Flat index.\n",
        "- **Cosine Similarity**: $\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}$ — ranges from -1 to 1; normalized versions range from 0 to 1. Better for comparing direction regardless of magnitude.\n",
        "\n",
        "**Implementation Note**:\n",
        "The cell below chooses `OpenAIEmbeddings` if the configured model name starts with `text-embedding` and falls back to `HuggingFaceEmbeddings` otherwise. Adjust `config['embeddingModelName']` to switch providers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lTp1YZHSsO4W"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model_name = config.get('embeddingModelName', 'text-embedding-3-small')\n",
        "\n",
        "if embedding_model_name.startswith(\"text-embedding\"):\n",
        "    # Use OpenAIEmbeddings for OpenAI models\n",
        "    embeddingFunction = OpenAIEmbeddings(model=embedding_model_name)\n",
        "else:\n",
        "    # Fallback to HuggingFaceEmbeddings for other models\n",
        "    embeddingFunction = HuggingFaceEmbeddings(model_name=embedding_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEmh-WX4vdjp"
      },
      "source": [
        "## Create Vector Store\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) is an in-memory index for efficient similarity search on dense vectors. It provides multiple index types optimized for different dataset sizes and recall-speed tradeoffs. For this RAG system, we use a flat L2 index (simple and effective for small-to-medium collections up to ~1M vectors).\n",
        "\n",
        "**Theoretical Foundation: Nearest-Neighbor Search as Retrieval**\n",
        "\n",
        "At its core, RAG retrieval is a nearest-neighbor search problem:\n",
        "1. Each document chunk is converted to a vector via embeddings: $\\mathbf{d}_i = \\text{embed}(\\text{chunk}_i)$.\n",
        "2. The user's query is converted to a vector: $\\mathbf{q} = \\text{embed}(\\text{question})$.\n",
        "3. The retriever finds the k chunks whose vectors are nearest to $\\mathbf{q}$ (i.e., smallest distance or highest cosine similarity).\n",
        "4. These k nearest-neighbor chunks are returned as the relevant context.\n",
        "\n",
        "The assumption: because the embedding space preserves semantic similarity, chunks with vectors near the query vector are likely semantically relevant to the question.\n",
        "\n",
        "**Creating the vector store**:\n",
        "- `FAISS.from_documents()` automatically embeds all chunks using your embedding function and builds an in-memory index.\n",
        "- The resulting vectorstore object provides a standard interface for similarity search, retrieval, and serialization.\n",
        "\n",
        "**FAISS Index Types** (for reference and future tuning):\n",
        "- **Flat (L2)**: exhaustive nearest-neighbor search; computes distance from query to all vectors. Best for small datasets (<100k vectors) and when exact recall is critical. Highest memory use per vector, but guaranteed to find true nearest neighbors.\n",
        "- **IVF (Inverted File)**: partitions vectors into k clusters (centroids) for faster approximate search; queries probe a subset of clusters. Ideal for medium datasets (100k–1M vectors). Trades some recall for speed; requires training on sample data.\n",
        "- **HNSW**: approximate nearest-neighbor using a hierarchical graph structure (inspired by Small-World networks); allows efficient search without exhaustive comparison. Excellent for large-scale (1M+ vectors) and fast retrieval with tunable recall-precision tradeoff.\n",
        "\n",
        "For more options and detailed [vector database benchmarks](https://docs.google.com/document/d/1RzLxisgBhFwciCNuztrgm5Y9CsHK7qxP0lYARH5ADo8/edit?usp=drive_link).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxg7kIaosMTK",
        "outputId": "b3d04776-1173-439a-f4da-ec3d97f4c6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store created successfully\n"
          ]
        }
      ],
      "source": [
        "# Create the FAISS vector store from our document chunks and embedding function\n",
        "vectorstore = FAISS.from_documents(chunks, embeddingFunction)\n",
        "print(\"Vector store created successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZwFMjFSZCUX",
        "outputId": "837476d6-8b7e-409c-94dc-4ba4c65c2bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
            "Number of vectors: 942\n",
            "Vector dimension: 1536\n"
          ]
        }
      ],
      "source": [
        "# Display basic statistics about the vector store\n",
        "print(f\"Vector store type: {type(vectorstore)}\")\n",
        "print(f\"Number of vectors: {vectorstore.index.ntotal}\")\n",
        "print(f\"Vector dimension: {vectorstore.index.d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd6TlUERZDnU",
        "outputId": "c7a00592-6c56-4c44-e5db-afba00237229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample vector (first 10 dimensions):\n",
            "[-0.02827102 -0.03220107  0.0256369  -0.01048718 -0.01531171  0.00960679\n",
            " -0.04927355 -0.03713124  0.01701614 -0.00686703]\n",
            "Vector shape: (1536,)\n"
          ]
        }
      ],
      "source": [
        "# Extract and view a sample vector (first 10 dimensions) to understand the data structure\n",
        "sample_vector = vectorstore.index.reconstruct(0)  # Get the first vector\n",
        "print(\"\\nSample vector (first 10 dimensions):\")\n",
        "print(sample_vector[:10])\n",
        "print(f\"Vector shape: {sample_vector.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRE4YPjSv4j_"
      },
      "source": [
        "## Save Vector Store Locally\n",
        "\n",
        "Persisting the vector store to disk allows you to reuse it across notebook runs, share indexes with teammates, and avoid recomputing expensive embeddings for the same document set. FAISS uses binary serialization (pickle format) for compact, fast I/O.\n",
        "\n",
        "**Saving best practices**:\n",
        "- Store the FAISS index alongside a metadata file (JSON or YAML) containing the embedding model name, creation date, chunk configuration, and source document info for full traceability.\n",
        "- Use version control or numbered backups (e.g., `faiss_index_v1/`, `faiss_index_v2/`) to track index evolution and enable rollback if needed.\n",
        "- On cloud storage (Google Drive, S3), use checksums and file locking to detect tampering or accidental overwrites.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI8YCwCoZkad",
        "outputId": "b2775479-ffb5-4f33-cfb7-f73a4158c1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CxlMpVMbgqdx"
      },
      "outputs": [],
      "source": [
        "# Define the path where the FAISS index will be saved\n",
        "gdrive_root = Path('/content/drive/MyDrive')\n",
        "faiss_dir = gdrive_root/\"teaching_fall_2025/LLM_Fall_2025/9_RAG/faiss_index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_HUEofDGhFUi"
      },
      "outputs": [],
      "source": [
        "# Ensure the directory exists\n",
        "faiss_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8EzOFdZhNmC",
        "outputId": "c2ed0e50-30b4-45e4-dfd6-08061cd02357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store saved to /content/drive/MyDrive/teaching_fall_2025/LLM_Fall_2025/9_RAG/faiss_index\n"
          ]
        }
      ],
      "source": [
        "# Save the vectorstore to Google Drive\n",
        "vectorstore.save_local(str(faiss_dir))\n",
        "print(f\"Vector store saved to {faiss_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti7i2XzXv-39"
      },
      "source": [
        "## Load Vector Store\n",
        "\n",
        "When loading a saved FAISS index, the `allow_dangerous_deserialization=True` parameter enables pickle deserialization, which poses a security risk if the index file comes from an untrusted source (pickle can execute arbitrary code during deserialization). \n",
        "\n",
        "**Security best practices when loading**:\n",
        "- Only load indexes from sources you control or trust (your own Google Drive, GitHub releases with GPG signatures, official team archives).\n",
        "- Always version and checksum your indexes so you can detect tampering (e.g., MD5 or SHA-256 hash stored separately).\n",
        "- If working with indexes from untrusted third parties, rebuild the index from raw documents instead of deserializing a pickled object.\n",
        "- Consider wrapping loads in try-except blocks to catch deserialization errors gracefully.\n",
        "\n",
        "By following these practices, you gain the performance and convenience of serialized indexes while minimizing security exposure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAHyvlZChStW",
        "outputId": "d29a009a-ab8f-42e8-c82f-326973e57fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store loaded successfully from Google Drive\n"
          ]
        }
      ],
      "source": [
        "# Load the vectorstore from Google Drive\n",
        "# 'allow_dangerous_deserialization' is required for pickle files, which FAISS uses\n",
        "loaded_vectorstore = FAISS.load_local(str(faiss_dir), embeddingFunction, allow_dangerous_deserialization=True)\n",
        "print(\"Vector store loaded successfully from Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h7PdTw8wJm5"
      },
      "source": [
        "## Explore vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "I5uLmpuCnYiZ"
      },
      "outputs": [],
      "source": [
        "# Get the internal mapping of FAISS IDs to Document IDs\n",
        "index_dict = loaded_vectorstore.index_to_docstore_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVcDgSBbn49S",
        "outputId": "64e2b0ee-0873-4440-96a8-0f18558c1026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First 10 items in the index_to_docstore_id dictionary:\n",
            "------------------------------------------------------------\n",
            "FAISS ID:   0 → Document ID: eb3dfc20-d125-43b9-a133-3b4b1d99cd93\n",
            "FAISS ID:   1 → Document ID: 771b9c55-a7d0-4dc8-86e6-cd2e80c5d5ca\n",
            "FAISS ID:   2 → Document ID: c63d11f7-0c28-4533-a527-a576166cca87\n",
            "FAISS ID:   3 → Document ID: 1025ae2f-0893-4fc4-92a8-e3a7add5924d\n",
            "FAISS ID:   4 → Document ID: 7a28d8a9-95c4-4ab3-8f8c-ef2315220b07\n",
            "FAISS ID:   5 → Document ID: 93a35c29-28a0-433d-ae39-f17c4b4db4d2\n",
            "FAISS ID:   6 → Document ID: f2bae244-6acc-47bd-bdee-2455b183fdfd\n",
            "FAISS ID:   7 → Document ID: 96021e99-edec-4808-81a8-e68ba77ebb17\n",
            "FAISS ID:   8 → Document ID: 659e35cf-ce61-40d8-b337-5b90bf325375\n",
            "FAISS ID:   9 → Document ID: 0f6019c3-a1c9-418b-b67c-4e2fc22ad5c5\n"
          ]
        }
      ],
      "source": [
        "# Display the first 10 items of this mapping\n",
        "print(f\"\\nFirst {min(10, len(index_dict))} items in the index_to_docstore_id dictionary:\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(min(10, len(index_dict))):\n",
        "    key = list(index_dict.keys())[i]\n",
        "    value = index_dict[key]\n",
        "    print(f\"FAISS ID: {key:>3} → Document ID: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex7C49jDhx-G",
        "outputId": "750cd7f4-ffb2-44b1-dae0-6e5d1d5296b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FAISS ID: 100\n",
            "Document ID: a89867d9-29d7-4e03-9ecc-614dd1439e93\n",
            "\n",
            "DOCUMENT METADATA:\n",
            "{'company': 'Tesla',\n",
            " 'language': 'No language found.',\n",
            " 'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm',\n",
            " 'title': 'tsla-20231231'}\n",
            "\n",
            "DOCUMENT CONTENT:\n",
            "('dangerous aspects of road travel much like the system that airplane pilots '\n",
            " 'use, when conditions permit. As with other vehicle systems, we improve these '\n",
            " 'functions in our vehicles over time through over-the-air updates.We intend '\n",
            " 'to establish in the future an autonomous Tesla ride-hailing network, which '\n",
            " 'we expect would also allow us to access a new customer base even as modes of '\n",
            " 'transportation evolve.We are also applying our artificial intelligence '\n",
            " 'learnings from self-driving technology to the')\n",
            "\n",
            "Vector dimensions: 1536\n",
            "\n",
            "First 10 values: [-0.00058303 -0.04598977  0.00645413  0.01947445 -0.00173879 -0.02887546\n",
            "  0.01028441  0.04728191  0.0040841   0.07594641]\n"
          ]
        }
      ],
      "source": [
        "# Detailed inspection of a specific chunk (e.g., ID 100)\n",
        "faiss_id = \"100\" if \"100\" in index_dict else 100\n",
        "\n",
        "# Retrieve Document ID\n",
        "document_id = loaded_vectorstore.index_to_docstore_id[faiss_id]\n",
        "print(f\"\\nFAISS ID: {faiss_id}\")\n",
        "print(f\"Document ID: {document_id}\")\n",
        "\n",
        "# Retrieve actual content and metadata\n",
        "document = loaded_vectorstore.docstore._dict[document_id]\n",
        "print(\"\\nDOCUMENT METADATA:\")\n",
        "pprint(document.metadata)\n",
        "print(\"\\nDOCUMENT CONTENT:\")\n",
        "pprint(document.page_content)\n",
        "\n",
        "# Retrieve the vector itself\n",
        "vector = loaded_vectorstore.index.reconstruct(int(faiss_id))\n",
        "print(f\"\\nVector dimensions: {len(vector)}\")\n",
        "print(f\"\\nFirst 10 values: {vector[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz6WT3UCicb7",
        "outputId": "c89b745b-bed4-446a-d18e-2635e6c46113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Similarity Search Results for Query ===\n",
            "What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\n",
            "\n",
            "Result 1 (Similarity Score: 0.8078051209449768):\n",
            "('Content: technology featuring three electric motors for further increased '\n",
            " 'performance in certain versions of Model S and Model X, Cybertruck and the '\n",
            " 'Tesla Semi.We maintain extensive testing and R&D capabilities for battery '\n",
            " 'cells, packs and systems, and have built an expansive body of knowledge on '\n",
            " 'lithium-ion cell chemistry types and performance characteristics. In order '\n",
            " 'to enable a greater supply of cells for our products with higher energy '\n",
            " 'density at lower costs, we have developed a new proprietary')\n",
            "(\"Metadata: {'source': \"\n",
            " \"'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', \"\n",
            " \"'title': 'tsla-20231231', 'language': 'No language found.', 'company': \"\n",
            " \"'Tesla'}\")\n",
            "\n",
            "Result 2 (Similarity Score: 0.9508378505706787):\n",
            "('Content: electric vehicles to address additional vehicle markets, and to '\n",
            " 'continue leveraging developments in our proprietary Full Self-Driving '\n",
            " '(“FSD”) Capability features, battery cell and other technologies.Energy '\n",
            " 'Generation and StorageEnergy Storage ProductsPowerwall and Megapack are our '\n",
            " 'lithium-ion battery energy storage products. Powerwall, which we sell '\n",
            " 'directly to customers, as well as through channel partners, is designed to '\n",
            " 'store energy at a home or small commercial facility. Megapack is an')\n",
            "(\"Metadata: {'source': \"\n",
            " \"'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', \"\n",
            " \"'title': 'tsla-20231231', 'language': 'No language found.', 'company': \"\n",
            " \"'Tesla'}\")\n",
            "\n",
            "Result 3 (Similarity Score: 0.9784986972808838):\n",
            "('Content: lower costs, we have developed a new proprietary lithium-ion '\n",
            " 'battery cell and improved manufacturing processes.Vehicle Control and '\n",
            " 'Infotainment SoftwareThe performance and safety systems of our vehicles and '\n",
            " 'their battery packs utilize sophisticated control software. Control systems '\n",
            " 'in our vehicles optimize performance, customize vehicle behavior, manage '\n",
            " 'charging and control all infotainment functions. We develop almost all of '\n",
            " 'this software, including most of the user interfaces, internally and')\n",
            "(\"Metadata: {'source': \"\n",
            " \"'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', \"\n",
            " \"'title': 'tsla-20231231', 'language': 'No language found.', 'company': \"\n",
            " \"'Tesla'}\")\n",
            "\n",
            "Result 4 (Similarity Score: 1.0036882162094116):\n",
            "('Content: PlansWe provide a manufacturer’s limited warranty on all new and '\n",
            " 'used Tesla vehicles we sell directly to consumers, which may include limited '\n",
            " 'warranties on certain components, specific types of damage or battery '\n",
            " 'capacity retention. We also currently offer optional extended service plans '\n",
            " 'that provide coverage beyond the new vehicle limited warranties for certain '\n",
            " 'models in specified regions.7Table of ContentsEnergy Generation and '\n",
            " 'StorageWe provide service and repairs to our energy product')\n",
            "(\"Metadata: {'source': \"\n",
            " \"'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', \"\n",
            " \"'title': 'tsla-20231231', 'language': 'No language found.', 'company': \"\n",
            " \"'Tesla'}\")\n",
            "\n",
            "Result 5 (Similarity Score: 1.0722628831863403):\n",
            "('Content: Electric Co., Ltd.10-Q001-3475610.6October 29, 201910.36††2020 '\n",
            " 'Pricing Agreement (Gigafactory 2170 Cells), entered into on June 9, 2020, by '\n",
            " 'and among Registrant, Tesla Motors Netherlands B.V., Panasonic Corporation '\n",
            " 'and Panasonic Corporation of North America.10-Q001-3475610.3July 28, '\n",
            " '202010.37††2021 Pricing Agreement (Japan Cells) with respect to 2011 Supply '\n",
            " 'Agreement, executed December 29, 2020, by and among the Registrant, Tesla '\n",
            " 'Motors Netherlands B.V., Panasonic Corporation of North America')\n",
            "(\"Metadata: {'source': \"\n",
            " \"'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', \"\n",
            " \"'title': 'tsla-20231231', 'language': 'No language found.', 'company': \"\n",
            " \"'Tesla'}\")\n",
            "\n",
            "Search complete!\n"
          ]
        }
      ],
      "source": [
        "# Perform a manual similarity search to see what chunks match our query\n",
        "query = \"What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\"\n",
        "print(f\"\\n=== Similarity Search Results for Query ===\\n{query}\")\n",
        "\n",
        "# search_results returns a list of (Document, score) tuples\n",
        "search_results = loaded_vectorstore.similarity_search_with_score(query, k=5)\n",
        "\n",
        "for i, (doc, score) in enumerate(search_results):\n",
        "    print(f\"\\nResult {i+1} (Similarity Score: {score}):\")\n",
        "    pprint(f\"Content: {doc.page_content}\")\n",
        "    pprint(f\"Metadata: {doc.metadata}\")\n",
        "\n",
        "print(\"\\nSearch complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se5wvN5BBmli"
      },
      "source": [
        "# Retrievers\n",
        "\n",
        "## Why Use a Retriever?\n",
        "\n",
        "A retriever serves as an abstraction layer that decouples the retrieval mechanism from the rest of your RAG pipeline. This provides multiple benefits:\n",
        "\n",
        "1. **Standardizes Access**: Provides a consistent interface (`invoke()` method) regardless of the underlying vector store implementation (FAISS, Pinecone, Weaviate, etc.).\n",
        "2. **Simplifies Integration**: Makes it seamless to plug into LangChain chains, agents, and other components without rewriting code when switching vector store providers.\n",
        "3. **Encapsulates Search Logic**: Hides implementation details of similarity search, filtering, reranking, and other retrieval strategies behind a simple public interface.\n",
        "4. **Enables Composition**: Retrievers can be stacked or combined (e.g., chaining multiple retrievers, adding filters, or reranking results) while maintaining a clean API.\n",
        "\n",
        "**In this notebook**, we create a retriever from our FAISS vector store using `.as_retriever()` and configure it with `search_kwargs={\"k\": 5}` to return the top-5 most similar documents. This retriever is then composed into the RAG chain to automatically fetch relevant context for every question.\n",
        "\n",
        "**Theoretical Insight: Relevance Ranking**\n",
        "\n",
        "When you call `retriever.invoke(question)`:\n",
        "1. The question is embedded into the same vector space as your document chunks: $\\mathbf{q} = \\text{embed}(\\text{question})$.\n",
        "2. The index computes a similarity score (e.g., L2 distance) between $\\mathbf{q}$ and each document vector: $\\text{score}_i = d(\\mathbf{q}, \\mathbf{d}_i)$ or $\\text{sim}(\\mathbf{q}, \\mathbf{d}_i)$.\n",
        "3. Results are ranked by score; the top-k documents (smallest distance or highest similarity) are returned.\n",
        "\n",
        "This ranking assumes that semantic similarity (as captured by embeddings) correlates with relevance. In practice, this often works well but isn't perfect—sometimes syntactically similar but semantically unrelated passages rank high, or relevant passages with different phrasing rank lower. Advanced techniques like reranking (using cross-encoders) can improve this, but for this notebook, we rely on embedding-based ranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XNIT9B9nES39"
      },
      "outputs": [],
      "source": [
        "# Create a retriever object from the vector store\n",
        "retriever = loaded_vectorstore.as_retriever(search_kwargs={\"k\": config[\"numRetrievedDocuments\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2QV_xXeEW9N",
        "outputId": "08f44aca-a9ee-4c55-9bc1-72bd5ad475ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7a5139aa58b0>, search_kwargs={'k': 5})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the retriever object\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NRLPVcF9EYaT"
      },
      "outputs": [],
      "source": [
        "# Test the retriever with our query\n",
        "retrieved_documents = retriever.invoke(\"What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiZy1JWMEs4R",
        "outputId": "65643595-a73d-4fd9-9a25-8312a4bed0b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='17098739-e7c7-4f6c-b746-458c6ce97e3c', metadata={'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', 'title': 'tsla-20231231', 'language': 'No language found.', 'company': 'Tesla'}, page_content='technology featuring three electric motors for further increased performance in certain versions of Model S and Model X, Cybertruck and the Tesla Semi.We maintain extensive testing and R&D capabilities for battery cells, packs and systems, and have built an expansive body of knowledge on lithium-ion cell chemistry types and performance characteristics. In order to enable a greater supply of cells for our products with higher energy density at lower costs, we have developed a new proprietary'),\n",
              " Document(id='778a4cb7-f62f-4be8-babb-49aa8c60c57b', metadata={'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', 'title': 'tsla-20231231', 'language': 'No language found.', 'company': 'Tesla'}, page_content='electric vehicles to address additional vehicle markets, and to continue leveraging developments in our proprietary Full Self-Driving (“FSD”) Capability features, battery cell and other technologies.Energy Generation and StorageEnergy Storage ProductsPowerwall and Megapack are our lithium-ion battery energy storage products. Powerwall, which we sell directly to customers, as well as through channel partners, is designed to store energy at a home or small commercial facility. Megapack is an'),\n",
              " Document(id='09cb8e3a-54b4-4565-908d-e722323d612c', metadata={'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', 'title': 'tsla-20231231', 'language': 'No language found.', 'company': 'Tesla'}, page_content='lower costs, we have developed a new proprietary lithium-ion battery cell and improved manufacturing processes.Vehicle Control and Infotainment SoftwareThe performance and safety systems of our vehicles and their battery packs utilize sophisticated control software. Control systems in our vehicles optimize performance, customize vehicle behavior, manage charging and control all infotainment functions. We develop almost all of this software, including most of the user interfaces, internally and'),\n",
              " Document(id='8cef5b90-622c-4916-8c60-24ea0914012e', metadata={'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', 'title': 'tsla-20231231', 'language': 'No language found.', 'company': 'Tesla'}, page_content='PlansWe provide a manufacturer’s limited warranty on all new and used Tesla vehicles we sell directly to consumers, which may include limited warranties on certain components, specific types of damage or battery capacity retention. We also currently offer optional extended service plans that provide coverage beyond the new vehicle limited warranties for certain models in specified regions.7Table of ContentsEnergy Generation and StorageWe provide service and repairs to our energy product'),\n",
              " Document(id='70afab94-d51a-4e89-b005-277541bf2c19', metadata={'source': 'https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm', 'title': 'tsla-20231231', 'language': 'No language found.', 'company': 'Tesla'}, page_content='Electric Co., Ltd.10-Q001-3475610.6October 29, 201910.36††2020 Pricing Agreement (Gigafactory 2170 Cells), entered into on June 9, 2020, by and among Registrant, Tesla Motors Netherlands B.V., Panasonic Corporation and Panasonic Corporation of North America.10-Q001-3475610.3July 28, 202010.37††2021 Pricing Agreement (Japan Cells) with respect to 2011 Supply Agreement, executed December 29, 2020, by and among the Registrant, Tesla Motors Netherlands B.V., Panasonic Corporation of North America')]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View the retrieved documents\n",
        "retrieved_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVzvbtOeHZRe"
      },
      "source": [
        "# Prepare inputs for Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX5NwFxRGA1s"
      },
      "source": [
        "## Document Formatter\n",
        "\n",
        "The document formatter function transforms raw retrieved documents into a structured format suitable for LLM consumption. \n",
        "\n",
        "**Why we need formatting**:\n",
        "- **Prevents Context Overflow**: By limiting results to `numSelectedDocuments` (default 5), we ensure the formatted context fits within the LLM's context window and reduces noise.\n",
        "- **Improves Readability**: Arranging chunks with clear separation and consistent formatting (company name, chunk text) helps the LLM parse and prioritize information.\n",
        "- **Maintains Traceability**: Preserving metadata like company names allows the LLM to cite sources and helps humans verify claims against original documents.\n",
        "- **Reduces Hallucination**: Clear, well-organized context encourages the LLM to cite supported facts rather than generate plausible-sounding but unsupported answers.\n",
        "\n",
        "**Implementation approach**:\n",
        "The `format_docs()` function below joins selected documents into a single string with company metadata and clear delimiters. This string is then inserted into the prompt template under the `{context}` placeholder, providing grounded context for answer generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RKLxukSDGEtk"
      },
      "outputs": [],
      "source": [
        "# Function to format documents into a string\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([\n",
        "        f\"{doc.metadata.get('company', '')}\\n{doc.page_content}\"\n",
        "        for doc in docs[:config[\"numSelectedDocuments\"]]\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyTpGjveGMYc",
        "outputId": "5a40ad95-72c0-491a-cb1c-2945e4a313c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Tesla\\n'\n",
            " 'technology featuring three electric motors for further increased performance '\n",
            " 'in certain versions of Model S and Model X, Cybertruck and the Tesla Semi.We '\n",
            " 'maintain extensive testing and R&D capabilities for battery cells, packs and '\n",
            " 'systems, and have built an expansive body of knowledge on lithium-ion cell '\n",
            " 'chemistry types and performance characteristics. In order to enable a '\n",
            " 'greater supply of cells for our products with higher energy density at lower '\n",
            " 'costs, we have developed a new proprietary\\n'\n",
            " '\\n'\n",
            " 'Tesla\\n'\n",
            " 'electric vehicles to address additional vehicle markets, and to continue '\n",
            " 'leveraging developments in our proprietary Full Self-Driving (“FSD”) '\n",
            " 'Capability features, battery cell and other technologies.Energy Generation '\n",
            " 'and StorageEnergy Storage ProductsPowerwall and Megapack are our lithium-ion '\n",
            " 'battery energy storage products. Powerwall, which we sell directly to '\n",
            " 'customers, as well as through channel partners, is designed to store energy '\n",
            " 'at a home or small commercial facility. Megapack is an\\n'\n",
            " '\\n'\n",
            " 'Tesla\\n'\n",
            " 'lower costs, we have developed a new proprietary lithium-ion battery cell '\n",
            " 'and improved manufacturing processes.Vehicle Control and Infotainment '\n",
            " 'SoftwareThe performance and safety systems of our vehicles and their battery '\n",
            " 'packs utilize sophisticated control software. Control systems in our '\n",
            " 'vehicles optimize performance, customize vehicle behavior, manage charging '\n",
            " 'and control all infotainment functions. We develop almost all of this '\n",
            " 'software, including most of the user interfaces, internally and\\n'\n",
            " '\\n'\n",
            " 'Tesla\\n'\n",
            " 'PlansWe provide a manufacturer’s limited warranty on all new and used Tesla '\n",
            " 'vehicles we sell directly to consumers, which may include limited warranties '\n",
            " 'on certain components, specific types of damage or battery capacity '\n",
            " 'retention. We also currently offer optional extended service plans that '\n",
            " 'provide coverage beyond the new vehicle limited warranties for certain '\n",
            " 'models in specified regions.7Table of ContentsEnergy Generation and '\n",
            " 'StorageWe provide service and repairs to our energy product\\n'\n",
            " '\\n'\n",
            " 'Tesla\\n'\n",
            " 'Electric Co., Ltd.10-Q001-3475610.6October 29, 201910.36††2020 Pricing '\n",
            " 'Agreement (Gigafactory 2170 Cells), entered into on June 9, 2020, by and '\n",
            " 'among Registrant, Tesla Motors Netherlands B.V., Panasonic Corporation and '\n",
            " 'Panasonic Corporation of North America.10-Q001-3475610.3July 28, '\n",
            " '202010.37††2021 Pricing Agreement (Japan Cells) with respect to 2011 Supply '\n",
            " 'Agreement, executed December 29, 2020, by and among the Registrant, Tesla '\n",
            " 'Motors Netherlands B.V., Panasonic Corporation of North America')\n"
          ]
        }
      ],
      "source": [
        "# Test the formatting function\n",
        "context = format_docs(retrieved_documents)\n",
        "pprint(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX5ufuvMJ8uU"
      },
      "source": [
        "## RunnablePassthrough()\n",
        "\n",
        "`RunnablePassthrough()` is a utility in LangChain's pipeline (LCEL - LangChain Expression Language) that acts as an identity function: it takes an input and passes it through unchanged to the next step in the chain.\n",
        "\n",
        "In your RAG pipeline, `RunnablePassthrough()` is used for the `\"question\"` key:\n",
        "```python\n",
        "{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "```\n",
        "\n",
        "This line says: \"Process the input (the question) through the retriever and formatter to get context, but also pass the original question directly to the prompt template.\"\n",
        "\n",
        "**Why we need RunnablePassthrough()**:\n",
        "\n",
        "In LangChain's pipeline architecture, when you need to process some keys of your input dictionary while leaving others unchanged, `RunnablePassthrough()` elegantly solves this:\n",
        "\n",
        "1. **Maintains Original Input**: Without `RunnablePassthrough()`, you'd lose the original question while processing it through the retriever. With it, both the processed context and the original question are available to the prompt.\n",
        "2. **Avoids Custom Functions**: Without it, you'd need to manually write a function to extract and preserve the question—`RunnablePassthrough()` handles this idiomatically.\n",
        "3. **Expresses Intent Clearly**: Reading `\"question\": RunnablePassthrough()` makes it explicit that the question should be forwarded unchanged, improving code readability and maintainability.\n",
        "4. **Enables Complex Pipelines**: As pipelines grow, you often need some inputs to be transformed (e.g., retrieval) and others to pass through (e.g., original query, metadata). `RunnablePassthrough()` scales this pattern cleanly.\n",
        "\n",
        "**Real-world analogy**: Think of a mail sorting facility. The address (question) needs to go through processing to find the correct recipient (retrieved context), but the original address also needs to stay on the envelope so the recipient knows who wrote it. `RunnablePassthrough()` is the mechanism that keeps the original address intact while the contents are processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxBSDCGdKBH6",
        "outputId": "71ba3cc4-cdf8-4108-e1a4-4a23d002003a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\n"
          ]
        }
      ],
      "source": [
        "# Example usage of RunnablePassthrough\n",
        "passthrough = RunnablePassthrough()\n",
        "question = passthrough.invoke(\"What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\")\n",
        "print(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVa-Jz5zGQ0c"
      },
      "source": [
        "# Prompt\n",
        "\n",
        "## Why Use LangChain's `PromptTemplate` Instead of String Formatting?\n",
        "\n",
        "While you could use simple Python f-strings or `.format()` for string templating, LangChain's `PromptTemplate` class provides important benefits:\n",
        "\n",
        "1. **Validation & Safety**: `PromptTemplate` validates that all required placeholders (e.g., `{question}`, `{context}`) are supplied at runtime. This catches missing variables early, reducing silent failures.\n",
        "\n",
        "2. **Serialization**: Prompts can be saved to JSON and loaded from disk, enabling version control and reproducibility of experiments across teams and time.\n",
        "\n",
        "3. **Integration with Components**: `PromptTemplate` objects are first-class citizens in LCEL pipelines, allowing seamless composition with retrievers, LLMs, and other components. This makes complex chains readable and maintainable.\n",
        "\n",
        "4. **Metadata & Documentation**: You can attach additional metadata to prompts (e.g., version, description, author) for better experiment tracking.\n",
        "\n",
        "5. **Variable Extraction**: The `.get_input_variables()` method automatically identifies required inputs, helping you debug pipeline mismatches.\n",
        "\n",
        "**In this notebook**, the `PromptTemplate` ensures your RAG prompt has exactly the structure needed to ground answers in the retrieved context and request clear, detailed responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yBDKiYGkG14H"
      },
      "outputs": [],
      "source": [
        "# Create the prompt template from the config string\n",
        "prompt_template = PromptTemplate.from_template(config[\"ragPromptTemplate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f9V-5iRG3ZI",
        "outputId": "d309a6ac-4291-40d6-dcd7-314569403b38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\\n    Provide a detailed answer with thorough explanations, avoiding summaries.\\n\\n    Question: {question}\\n\\n    Context: {context}\\n\\n    Answer:\\n    ')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inspect the template object\n",
        "prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJgnvxIWHunI",
        "outputId": "48671d50-c7a4-437b-9df2-024f5a33a63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\n",
            "    Provide a detailed answer with thorough explanations, avoiding summaries.\n",
            "\n",
            "    Question: What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\n",
            "\n",
            "    Context: Tesla\n",
            "technology featuring three electric motors for further increased performance in certain versions of Model S and Model X, Cybertruck and the Tesla Semi.We maintain extensive testing and R&D capabilities for battery cells, packs and systems, and have built an expansive body of knowledge on lithium-ion cell chemistry types and performance characteristics. In order to enable a greater supply of cells for our products with higher energy density at lower costs, we have developed a new proprietary\n",
            "\n",
            "Tesla\n",
            "electric vehicles to address additional vehicle markets, and to continue leveraging developments in our proprietary Full Self-Driving (“FSD”) Capability features, battery cell and other technologies.Energy Generation and StorageEnergy Storage ProductsPowerwall and Megapack are our lithium-ion battery energy storage products. Powerwall, which we sell directly to customers, as well as through channel partners, is designed to store energy at a home or small commercial facility. Megapack is an\n",
            "\n",
            "Tesla\n",
            "lower costs, we have developed a new proprietary lithium-ion battery cell and improved manufacturing processes.Vehicle Control and Infotainment SoftwareThe performance and safety systems of our vehicles and their battery packs utilize sophisticated control software. Control systems in our vehicles optimize performance, customize vehicle behavior, manage charging and control all infotainment functions. We develop almost all of this software, including most of the user interfaces, internally and\n",
            "\n",
            "Tesla\n",
            "PlansWe provide a manufacturer’s limited warranty on all new and used Tesla vehicles we sell directly to consumers, which may include limited warranties on certain components, specific types of damage or battery capacity retention. We also currently offer optional extended service plans that provide coverage beyond the new vehicle limited warranties for certain models in specified regions.7Table of ContentsEnergy Generation and StorageWe provide service and repairs to our energy product\n",
            "\n",
            "Tesla\n",
            "Electric Co., Ltd.10-Q001-3475610.6October 29, 201910.36††2020 Pricing Agreement (Gigafactory 2170 Cells), entered into on June 9, 2020, by and among Registrant, Tesla Motors Netherlands B.V., Panasonic Corporation and Panasonic Corporation of North America.10-Q001-3475610.3July 28, 202010.37††2021 Pricing Agreement (Japan Cells) with respect to 2011 Supply Agreement, executed December 29, 2020, by and among the Registrant, Tesla Motors Netherlands B.V., Panasonic Corporation of North America\n",
            "\n",
            "    Answer:\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Preview the final formatted prompt with actual data\n",
        "# This ensures context is defined even if cells are run out of order\n",
        "context = format_docs(retrieved_documents)\n",
        "formatted_prompt = prompt_template.format(question=question, context=context)\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTsDqr-HHB2K"
      },
      "source": [
        "# LLM for Answer Generation\n",
        "\n",
        "The LLM (Large Language Model) is the reasoning engine of your RAG system. It reads the retrieved context and the user's question, then generates a grounded answer based on what the documents contain.\n",
        "\n",
        "**Theoretical Foundation: Attention and Language Generation**\n",
        "\n",
        "Modern LLMs (like GPT-4) are transformer-based models that use the **attention mechanism** to:\n",
        "1. **Understand Context**: Given the prompt (question + retrieved passages), the model uses self-attention to compute relevance weights between all token pairs, allowing it to \"focus\" on the most important information.\n",
        "2. **Reason Over Evidence**: The attention mechanism allows the model to trace dependencies and synthesize information across multiple passages.\n",
        "3. **Generate Answers**: Using these learned representations, the model generates the next token (word) based on the previous tokens, repeating until a stopping criterion (e.g., end-of-sequence token).\n",
        "\n",
        "Mathematically, attention computes: $\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}$, where queries, keys, and values are derived from the input tokens. This allows the model to dynamically weight which parts of the input to pay attention to.\n",
        "\n",
        "**Why Use `ChatOpenAI` in This Notebook**:\n",
        "- **High Quality**: GPT-4 and similar models provide excellent reasoning, understanding of technical documents, and the ability to synthesize information across multiple sources.\n",
        "- **Managed Service**: No need to run or fine-tune a model locally; just provide an API key and let OpenAI handle infrastructure, scaling, and updates.\n",
        "- **Context Understanding**: These models excel at following instructions (e.g., \"use only the provided context\") and maintaining consistency within a conversation.\n",
        "- **Instruction Following**: Modern LLMs respond well to explicit instructions in prompts, enabling better control over answer style and factuality.\n",
        "\n",
        "**Key Configuration Parameters**:\n",
        "- `model`: Model identifier (e.g., `\"gpt-4o\"`) determines capability level. Larger models (gpt-4) are more capable but costlier; smaller models (gpt-3.5-turbo) are faster and cheaper.\n",
        "- `temperature`: Controls randomness in generation. \n",
        "  - `0.0` → deterministic, factual, reproducible (best for RAG when you want answers directly from context).\n",
        "  - `0.7` → balanced creativity and consistency (suitable for open-ended questions).\n",
        "  - `1.0` → highly creative but less predictable (risky for fact-based tasks).\n",
        "\n",
        "**For RAG Systems**: prefer lower temperatures (0.0–0.3) to encourage the LLM to stick to retrieved facts and reduce hallucinations. The attention mechanism combined with low temperature makes the model more likely to rely on factual content in the context rather than generating plausible-sounding but unsupported claims.\n",
        "\n",
        "The cell below initializes `ChatOpenAI` with your configured model and temperature, preparing it for use in the RAG chain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Q6BeiJD4HUDT"
      },
      "outputs": [],
      "source": [
        "# Initialize the Chat OpenAI model\n",
        "llm = ChatOpenAI(model=config[\"ragAnswerModel\"], temperature=config[\"ragAnswerModelTemeprature\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_np-cJvHVOi",
        "outputId": "ec084f08-c457-4c65-fc05-9098b4efabb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x7a5139aa6d50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7a5135e1d130>, root_client=<openai.OpenAI object at 0x7a5139aa61e0>, root_async_client=<openai.AsyncOpenAI object at 0x7a5139aa50a0>, model_name='gpt-4o', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify model configuration\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6fZDsApUHYpP"
      },
      "outputs": [],
      "source": [
        "# Generate the answer by invoking the LLM with the formatted prompt\n",
        "llm_output = llm.invoke(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mFgAajAMy8T",
        "outputId": "3743b0d8-9d6c-430f-a857-d31d3836fc17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Tesla has made several technological advancements in the batteries used in its electric vehicles, primarily focusing on lithium-ion battery cells. The company has developed a new proprietary lithium-ion battery cell that offers higher energy density at lower costs. This advancement is crucial as it directly impacts the range, performance, and overall efficiency of Tesla's electric vehicles.\\n\\nFurthermore, Tesla has improved its manufacturing processes, which likely contribute to the increased efficiency and reduced costs associated with their battery production. This indicates a focus not only on the chemistry of the battery cells themselves but also on the way they are manufactured, allowing Tesla to scale production and meet the growing demand for their vehicles.\\n\\nTesla's extensive research and development capabilities are evident in their comprehensive testing of battery cells, packs, and systems. This has allowed them to build a significant body of knowledge regarding lithium-ion cell chemistry and performance characteristics, which they utilize to enhance their battery technology continually.\\n\\nAdditionally, Tesla's vehicles utilize sophisticated control software that optimizes the performance and safety systems of the vehicles and their battery packs. This software manages charging, controls all infotainment functions, and customizes vehicle behavior, demonstrating Tesla's integrated approach to vehicle technology and battery management.\\n\\nOverall, Tesla's advancements in battery technology are multifaceted, involving the development of proprietary battery cell technology, improved manufacturing processes, and sophisticated software that optimizes battery use and vehicle performance. These innovations are central to Tesla's ability to offer electric vehicles that are efficient, cost-effective, and high-performing.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 293, 'prompt_tokens': 555, 'total_tokens': 848, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e819e3438b', 'id': 'chatcmpl-ChRgt4YP9rjkZ2ZvWtqEv0HlGbMPl', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--b034039e-a31c-4170-be1e-f83e18a6c620-0', usage_metadata={'input_tokens': 555, 'output_tokens': 293, 'total_tokens': 848, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View the raw output object (AIMessage)\n",
        "llm_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RYrjK6hMZCo",
        "outputId": "e5516c18-aa93-4542-97bd-d56175b403e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Tesla has made several technological advancements in the batteries used in '\n",
            " 'its electric vehicles, primarily focusing on lithium-ion battery cells. The '\n",
            " 'company has developed a new proprietary lithium-ion battery cell that offers '\n",
            " 'higher energy density at lower costs. This advancement is crucial as it '\n",
            " \"directly impacts the range, performance, and overall efficiency of Tesla's \"\n",
            " 'electric vehicles.\\n'\n",
            " '\\n'\n",
            " 'Furthermore, Tesla has improved its manufacturing processes, which likely '\n",
            " 'contribute to the increased efficiency and reduced costs associated with '\n",
            " 'their battery production. This indicates a focus not only on the chemistry '\n",
            " 'of the battery cells themselves but also on the way they are manufactured, '\n",
            " 'allowing Tesla to scale production and meet the growing demand for their '\n",
            " 'vehicles.\\n'\n",
            " '\\n'\n",
            " \"Tesla's extensive research and development capabilities are evident in their \"\n",
            " 'comprehensive testing of battery cells, packs, and systems. This has allowed '\n",
            " 'them to build a significant body of knowledge regarding lithium-ion cell '\n",
            " 'chemistry and performance characteristics, which they utilize to enhance '\n",
            " 'their battery technology continually.\\n'\n",
            " '\\n'\n",
            " \"Additionally, Tesla's vehicles utilize sophisticated control software that \"\n",
            " 'optimizes the performance and safety systems of the vehicles and their '\n",
            " 'battery packs. This software manages charging, controls all infotainment '\n",
            " \"functions, and customizes vehicle behavior, demonstrating Tesla's integrated \"\n",
            " 'approach to vehicle technology and battery management.\\n'\n",
            " '\\n'\n",
            " \"Overall, Tesla's advancements in battery technology are multifaceted, \"\n",
            " 'involving the development of proprietary battery cell technology, improved '\n",
            " 'manufacturing processes, and sophisticated software that optimizes battery '\n",
            " \"use and vehicle performance. These innovations are central to Tesla's \"\n",
            " 'ability to offer electric vehicles that are efficient, cost-effective, and '\n",
            " 'high-performing.')\n"
          ]
        }
      ],
      "source": [
        "# View just the content string\n",
        "pprint(llm_output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj4GyPdbLqmG"
      },
      "source": [
        "# PostProcessing\n",
        "\n",
        "## Output Parsing and String Conversion\n",
        "\n",
        "`StrOutputParser()` is the final component in the RAG chain that converts the LLM's structured output into a clean, readable string. Without this parser, the LLM returns an `AIMessage` object containing metadata (like token count, model name, finish reason) along with the text content. `StrOutputParser()` extracts just the text, discarding the wrapper.\n",
        "\n",
        "**In the RAG chain**, `StrOutputParser()` is the last step:\n",
        "```python\n",
        "rag_chain = (...) | StrOutputParser()\n",
        "```\n",
        "\n",
        "When you call `rag_chain.invoke(question)`, the result is a plain string ready for display, rather than a complex object with embedded metadata.\n",
        "\n",
        "**Why Output Parsing Matters**:\n",
        "- **User Experience**: Users expect plain text answers, not Python objects or JSON.\n",
        "- **Integration**: Downstream systems (web APIs, frontends, logging) expect strings or simple data types.\n",
        "- **Composability**: In complex chains, output parsers enable type-safe data flow and prevent errors when chaining components.\n",
        "\n",
        "## Evaluating RAG System Quality\n",
        "\n",
        "To improve your RAG system, measure its performance on a test set:\n",
        "\n",
        "1. **Retrieval Quality** (does the retriever find relevant passages?):\n",
        "   - Precision@k: % of top-k retrieved docs that are relevant\n",
        "   - Recall: % of all relevant docs in the corpus that appear in top-k results\n",
        "   - MRR (Mean Reciprocal Rank): average rank of the first relevant document\n",
        "   - Tool: Run `retriever.invoke(question)` for each test query and manually assess relevance.\n",
        "\n",
        "2. **Answer Quality** (does the LLM give correct, grounded answers?):\n",
        "   - F1 Score: overlap between generated answer and reference answer(s)\n",
        "   - Factuality: % of claims in the answer that are directly supported by retrieved docs\n",
        "   - Hallucination Rate: % of claims not in the source context\n",
        "   - User satisfaction: ask domain experts to rate answer quality on a scale\n",
        "\n",
        "3. **End-to-End Quality**:\n",
        "   - Compare answers with/without retrieval (should be better with RAG).\n",
        "   - Baseline against traditional search or closed-book LLM answers.\n",
        "\n",
        "**Theoretical Note: The Hallucination Problem**\n",
        "\n",
        "Even with retrieval context, LLMs can \"hallucinate\"—generate false or unsupported information. This happens because:\n",
        "- The model's training data contains patterns that generate plausible-sounding text.\n",
        "- The model may overgeneralize from the context or inject knowledge from its training set.\n",
        "- Attention weights may focus on context tokens that don't directly answer the question.\n",
        "\n",
        "RAG reduces (but doesn't eliminate) hallucinations by grounding answers in retrieved documents. Using low temperature and explicit instructions (\"only use the provided context\") further helps.\n",
        "\n",
        "## Quick Debugging Checklist\n",
        "\n",
        "If answers are poor:\n",
        "- **Check retrieval**: print retrieved docs to see if relevant passages are being found.\n",
        "- **Adjust config**: try different `chunkSize`, `chunkOverlap`, `numRetrievedDocuments`, or `temperature`.\n",
        "- **Verify embeddings**: ensure you're using the same embedding model for indexing and querying.\n",
        "- **Test prompt clarity**: modify `ragPromptTemplate` to give clearer instructions to the LLM.\n",
        "\n",
        "## Next Steps for Learning\n",
        "\n",
        "- **Experiment with different documents**: load a different SEC filing or article and re-run the pipeline.\n",
        "- **Implement reranking**: after retrieval, rerank the top-k docs using a cross-encoder to improve precision.\n",
        "- **Add filtering**: pre-filter chunks by date, section, or metadata before sending to the LLM.\n",
        "- **Try different LLMs**: replace `ChatOpenAI` with open-source models (Llama, Mistral) or other providers (Anthropic, Cohere).\n",
        "- **Optimize costs**: use smaller, cheaper embedding/LLM models if performance allows.\n",
        "- **Deploy**: wrap the chain in a web service (FastAPI, Streamlit) for real-world use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "52kDLBntMSID"
      },
      "outputs": [],
      "source": [
        "# Initialize the output parser\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lyRwCLLIMTxf"
      },
      "outputs": [],
      "source": [
        "# Parse the LLM output\n",
        "final_output = output_parser.invoke(llm_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suTCfQzGNGu_",
        "outputId": "2eef1731-67e7-482e-fa0a-70a38edf69b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Tesla has made several technological advancements in the batteries used in '\n",
            " 'its electric vehicles, primarily focusing on lithium-ion battery cells. The '\n",
            " 'company has developed a new proprietary lithium-ion battery cell that offers '\n",
            " 'higher energy density at lower costs. This advancement is crucial as it '\n",
            " \"directly impacts the range, performance, and overall efficiency of Tesla's \"\n",
            " 'electric vehicles.\\n'\n",
            " '\\n'\n",
            " 'Furthermore, Tesla has improved its manufacturing processes, which likely '\n",
            " 'contribute to the increased efficiency and reduced costs associated with '\n",
            " 'their battery production. This indicates a focus not only on the chemistry '\n",
            " 'of the battery cells themselves but also on the way they are manufactured, '\n",
            " 'allowing Tesla to scale production and meet the growing demand for their '\n",
            " 'vehicles.\\n'\n",
            " '\\n'\n",
            " \"Tesla's extensive research and development capabilities are evident in their \"\n",
            " 'comprehensive testing of battery cells, packs, and systems. This has allowed '\n",
            " 'them to build a significant body of knowledge regarding lithium-ion cell '\n",
            " 'chemistry and performance characteristics, which they utilize to enhance '\n",
            " 'their battery technology continually.\\n'\n",
            " '\\n'\n",
            " \"Additionally, Tesla's vehicles utilize sophisticated control software that \"\n",
            " 'optimizes the performance and safety systems of the vehicles and their '\n",
            " 'battery packs. This software manages charging, controls all infotainment '\n",
            " \"functions, and customizes vehicle behavior, demonstrating Tesla's integrated \"\n",
            " 'approach to vehicle technology and battery management.\\n'\n",
            " '\\n'\n",
            " \"Overall, Tesla's advancements in battery technology are multifaceted, \"\n",
            " 'involving the development of proprietary battery cell technology, improved '\n",
            " 'manufacturing processes, and sophisticated software that optimizes battery '\n",
            " \"use and vehicle performance. These innovations are central to Tesla's \"\n",
            " 'ability to offer electric vehicles that are efficient, cost-effective, and '\n",
            " 'high-performing.')\n"
          ]
        }
      ],
      "source": [
        "# Print the final clean result\n",
        "pprint(final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xOFgE0nyB1c"
      },
      "source": [
        "# RAG Chain\n",
        "\n",
        "## RAG Chain Overview\n",
        "\n",
        "The RAG chain is a composable pipeline that orchestrates all components—retriever, formatter, prompt, and LLM—into a single callable unit. Using LCEL (LangChain Expression Language), you can express the entire pipeline as a linear sequence, making the flow clear and easy to modify.\n",
        "\n",
        "## RAG Chain Components and Data Flow\n",
        "\n",
        "1. **Input**: User's question (string)\n",
        "2. **Retriever**: Fetches top-k most similar documents based on semantic similarity to the question.\n",
        "3. **Document Formatter**: Transforms retrieved documents into a structured text block with company metadata and clear delimiters.\n",
        "4. **Prompt Template**: Combines the formatted context and original question into a structured prompt string with explicit instructions (e.g., \"use only provided context\").\n",
        "5. **LLM**: Generates a grounded answer based on the prompt.\n",
        "6. **Output Parser**: Extracts the text response from the LLM's structured output (AIMessage object).\n",
        "7. **Output**: Final answer string ready for display to the user.\n",
        "\n",
        "## LCEL Pipeline Syntax\n",
        "\n",
        "The RAG chain is built using LCEL's pipe operator (`|`):\n",
        "\n",
        "```python\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "```\n",
        "\n",
        "**Reading this syntax**:\n",
        "- `{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}`: Create a dictionary with two keys. For \"context\", pass the input through the retriever, then through `format_docs()`. For \"question\", pass the input unchanged via `RunnablePassthrough()`.\n",
        "- `| prompt`: Pass the resulting dictionary to the prompt template, which substitutes `{context}` and `{question}` placeholders.\n",
        "- `| llm`: Send the formatted prompt to the LLM.\n",
        "- `| StrOutputParser()`: Extract the text response.\n",
        "\n",
        "**Key insight**: Each `|` represents a handoff between components. Data flows left-to-right, with each component's output becoming the next component's input.\n",
        "\n",
        "## Why This Design?\n",
        "\n",
        "- **Modularity**: Each component (retriever, formatter, prompt, LLM) is independently testable and replaceable.\n",
        "- **Readability**: The LCEL syntax reads like a narrative description of the process.\n",
        "- **Composability**: You can build complex pipelines by combining simple components without nesting function calls.\n",
        "- **Debuggability**: You can invoke intermediate steps (e.g., `retriever.invoke(question)`) to inspect behavior and troubleshoot issues.\n",
        "\n",
        "## Usage and Reproducibility\n",
        "\n",
        "Once the chain is built, invoke it with your question:\n",
        "\n",
        "```python\n",
        "answer = rag_chain.invoke(\"Your question here?\")\n",
        "```\n",
        "\n",
        "The same chain can answer multiple questions without reinitialization. For reproducible results (especially important in research), keep `temperature` low and document your config settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3qPMDemyMEz",
        "outputId": "9fb5c501-af65-47a5-a39b-58b38f2ccaa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} template='\\n    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\\n    Provide a detailed answer with thorough explanations, avoiding summaries.\\n\\n    Question: {question}\\n\\n    Context: {context}\\n\\n    Answer:\\n    '\n"
          ]
        }
      ],
      "source": [
        "# Re-creating components for clarity in the final chain definition\n",
        "\n",
        "# 1. Retriever\n",
        "retriever = loaded_vectorstore.as_retriever(search_kwargs={\"k\": config[\"numRetrievedDocuments\"]})\n",
        "\n",
        "# 2. Formatting function (re-defined or reused)\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([\n",
        "        f\"{doc.metadata.get('company', '')}\\n{doc.page_content}\"\n",
        "        for doc in docs[:config[\"numSelectedDocuments\"]]\n",
        "    ])\n",
        "\n",
        "# 3. LLM\n",
        "llm = ChatOpenAI(model=config[\"ragAnswerModel\"], temperature=config[\"ragAnswerModelTemeprature\"])\n",
        "\n",
        "# 4. Prompt\n",
        "prompt = PromptTemplate.from_template(config[\"ragPromptTemplate\"])\n",
        "print(prompt)\n",
        "\n",
        "# 5. Build the RAG Chain using LCEL\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIUSGAihyB1c"
      },
      "source": [
        "# Ask a Question\n",
        "\n",
        "Now let's invoke the complete RAG chain with a concrete question about Tesla's 10-K filing. The chain will:\n",
        "1. Retrieve the 5 most relevant passages from the document.\n",
        "2. Format them into a single context block.\n",
        "3. Construct a prompt combining context and question.\n",
        "4. Send the prompt to GPT-4o for answer generation.\n",
        "5. Parse and return the final answer.\n",
        "\n",
        "**Expected behavior**: The answer should be grounded in the retrieved passages, avoiding hallucinations. If the source document doesn't contain relevant information, the LLM should indicate that rather than inventing an answer.\n",
        "\n",
        "**Debugging tip**: If the answer doesn't match your expectations, inspect the retrieved documents (uncomment the debug cells below) to see whether the retriever found relevant passages before blaming the LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo2UoQhLyB1c",
        "outputId": "27416319-bbee-483d-f678-c46371dd3574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla has made several technological advancements in the batteries used in their electric vehicles, focusing on lithium-ion cell chemistry and performance characteristics. These advancements include the development of a new proprietary lithium-ion battery cell that aims to offer higher energy density at lower costs. This involves improved manufacturing processes that enhance the efficiency and performance of the battery cells.\n",
            "\n",
            "Tesla's extensive testing and research and development (R&D) capabilities have allowed them to build a comprehensive understanding of different lithium-ion cell chemistries, which is crucial for optimizing the performance of their battery packs. These battery packs are integral to the performance and safety systems of Tesla vehicles. They utilize sophisticated control software to optimize performance, manage charging, and customize vehicle behavior.\n",
            "\n",
            "Additionally, Tesla's battery advancements are reflected in their energy storage products, such as the Powerwall and Megapack. These products are designed to store energy efficiently for residential or small commercial use, indicating that the advancements in battery technology are not only applied to their vehicles but also extend to their energy generation and storage solutions.\n",
            "\n",
            "Overall, the advancements in Tesla's battery technology focus on increasing energy density, reducing costs, and improving manufacturing processes, all while leveraging their in-depth knowledge of lithium-ion chemistry and software development to enhance the performance and safety of their battery systems.\n"
          ]
        }
      ],
      "source": [
        "# Define the question\n",
        "question = \"What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\"\n",
        "\n",
        "# Invoke the chain\n",
        "answer = rag_chain.invoke(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV6ldo9TyB1c"
      },
      "source": [
        "# Try Other Questions\n",
        "\n",
        "Use the cell below to test the RAG system with different questions. The same `rag_chain` can answer multiple queries without reinitialization, provided the vector store and configuration remain unchanged.\n",
        "\n",
        "**Experiment ideas**:\n",
        "- Ask questions that are directly answered in the 10-K (should return high-quality answers).\n",
        "- Ask questions the document doesn't address (the LLM should acknowledge this rather than hallucinate).\n",
        "- Ask follow-up questions that require synthesizing information across multiple sections.\n",
        "- Modify `config['numRetrievedDocuments']` (higher values = more context, potentially better answers but also more noise) and rerun cells to observe effects.\n",
        "\n",
        "**For consistent results across runs**: keep `ragAnswerModelTemperature` at 0.0–0.3. Higher temperatures will produce varied responses even with identical inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "Pgi2eBssyB1c",
        "outputId": "3cf6defd-55ff-47d8-cbde-757986071d69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Tesla's main revenue sources are primarily categorized into three segments: Automotive Sales, Energy Generation and Storage, and Services and Other.\\n\\n1. **Automotive Sales**: This is the largest revenue source for Tesla. It includes the sale of new vehicles such as the Model S, Model X, Model 3, Model Y, Cybertruck, and Semi. The revenue from automotive sales encompasses both cash and financed deliveries. It also includes revenue from the sale of optional features and services such as Full Self-Driving (FSD) capabilities, over-the-air software updates, internet connectivity, and other subscriptions or additional features that can be purchased through the Tesla app or in-vehicle interface.\\n\\n2. **Automotive Regulatory Credits**: Although part of the automotive segment, it's worth mentioning separately due to its significance. Tesla earns regulatory credits by producing zero-emission vehicles, which it can then sell to other manufacturers that need these credits to comply with environmental regulations. This is a notable revenue stream that contributes to Tesla's overall automotive revenues.\\n\\n3. **Automotive Leasing**: This includes revenue from leasing Tesla vehicles. Leasing allows customers to use Tesla vehicles for a period, providing a recurring revenue stream for Tesla over the lease term.\\n\\n4. **Energy Generation and Storage**: This segment involves the sale of solar energy systems and energy storage products to residential, commercial, and industrial customers. Tesla markets these products through various channels, including its website, stores, and galleries. The energy segment has been experiencing growth, contributing a significant share to Tesla's total revenues.\\n\\n5. **Services and Other**: This includes revenue from various services Tesla provides, like vehicle maintenance, repairs, and public charging through its network of Superchargers. Additionally, it covers revenue from the sale of used Tesla vehicles, trade-ins, and remarketing of vehicles acquired from lease returns or other sources.\\n\\nEach of these segments contributes to Tesla's overall financial performance, with automotive sales being the most substantial, followed by energy generation and storage and services and other.\""
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Try another question to test generalization\n",
        "another_question = \"What are Tesla's main revenue sources?\"\n",
        "rag_chain.invoke(another_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE_B_1PnrQq9"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
